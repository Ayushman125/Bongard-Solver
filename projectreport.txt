## Concept Registry and Coverage Enforcement
data generation for phase 1

A. Central Concept Registry: Self-Maintaining, Auto-Inducing, and Auditable
A new self-maintaining concept registry (`src/concepts/registry.py`) is implemented. This registry automatically induces and caches a concept-checking predicate for every problem in the dataset, using only the features and labels in `derived_labels.json`. No manual wiring or fallback logic is required—coverage is guaranteed for all present and future problems.

#### How It Works
- On first run, the registry loads all problem IDs from `derived_labels.json`.
- For each problem, it extracts the features for positives (`category_1`) and negatives (`category_0`).
- It tries a library of simple, interpretable predicates (e.g., `is_convex`, `num_straight==n`, `has_quadrangle`, etc.) and selects the first that perfectly separates positives from negatives.
- The discovered predicate is cached in `data/concepts_auto.yaml` for auditability and determinism.
- On subsequent runs, the registry loads the cache and uses the stored predicate as the concept function for that problem.
- If new problems appear, the registry induces and caches their rules on the fly, with no crashes or manual edits required.

#### Guarantees
- **Soundness**: Each predicate is chosen only if it separates all supports in the data.
- **Determinism**: The cache file guarantees identical behavior across runs.
- **Auditability**: Each YAML entry records the exact predicate and parameter; reviewers can inspect or override as needed.
- **Performance**: Induction is fast (milliseconds per problem) and negligible at runtime.
- **Extensibility**: Add a new template in one line in `auto_inducer.py` to cover new concept types.

#### Integration
- The hard negative mining pipeline (`scripts/generate_hard_negatives.py`) uses the registry and factory logic. All fallback logic is removed. If a problem is not covered, the process fails fast, ensuring data integrity and full concept coverage.
- The registry is fully compatible with future dataset expansions—new problems are handled automatically.


B. **Hard Negative Mining Pipeline: Modern, Multi-Strategy, and Fully Automated**

#### 1. Multi-Strategy Hard Negative Generation

The hard negative mining pipeline now integrates **five advanced, research-backed strategies** to guarantee both the *quantity* and *quality* of hard negatives for every Bongard-LOGO problem:

- **Deterministic Concept Inversions:**
  For each problem, a registry of per-problem, concept-aware inversion functions is maintained (`src/hard_negative/concept_inversions.py`). These are applied first to generate guaranteed label-flipping negatives for symmetry, arrangement, and other resistant concepts.

- **Metamorphic Affine Inversion Testing (MAIT):**
  A suite of closed-form affine transforms (rotation, scaling, shear, translation) is applied to each positive sample. If the concept function’s output shifts by more than a threshold, the result is recorded as a hard negative. See `src/data_pipeline/affine_transforms.py`.

- **Procedural Shape Perturbation Ensemble (PSPE):**
  Five procedural routines (Perlin jitter, subdivision, wave distortion, radial perturbation, noise scaling) are run in parallel on each sample, each producing a distinct adversarial negative. See `src/data_pipeline/procedural.py`.

- **Geometry-Aware GAN Mining (GAGAN-HNM):**
  A shape-domain GAN is used to generate 1000+ candidate perturbations per sample. The concept function acts as a discriminator, and all label-flipping outputs are retained. See `src/hard_negative/gagan_model.py`.

- **Hyperbolic Hard-Negative Mixup (HHNM):**
  Features are embedded in both Euclidean and hyperbolic space. Top-k hard negatives are mined in each space, and synthetic negatives are created via Poincaré ball mixup. See `src/data_pipeline/hyperbolic.py`.

All strategies are orchestrated in a single, O(1) pipeline in `scripts/generate_hard_negatives.py` (see `process_sample_with_guaranteed_success`). The pipeline is fully parallelizable and deterministic (all random seeds are set via CLI).

#### 2. Deduplication, Geometry Validity, and Diversity

- **Program-Level Deduplication:**
  Each candidate is hashed by its full action program and problem ID to ensure no duplicates are saved.

- **Geometry Validity:**
  All negatives are checked for geometric validity (no self-intersections, minimum vertex count) before being accepted.

- **Diversity Filtering:**
  Embedding-based filtering (L2 distance in feature space) ensures that only diverse negatives are retained, avoiding redundancy.

#### 3. Full Auditability and Coverage

- **Concept Registry:**
  The concept registry (`src/concepts/registry.py`) guarantees that every problem has a valid, auditable concept function. If a new problem appears, the registry auto-induces a separating predicate and caches it for future runs.

- **Logging and Traceability:**
  Every negative logs its mutator chain, fitness score, and the strategy that produced it. All label values and sample selection steps are logged for auditability.

- **Parallel Execution:**
  The pipeline supports multi-core execution for rapid mining of 600–900 hard negatives in minutes.

#### 4. Output and Integration

- **Output Files:**
  - `data/hard_negatives.txt`: List of all generated hard negatives, with full metadata.
  - `data/flagged_cases.txt`: Any samples failing geometry or label checks.
  - `data/derived_labels.json`: All computed features and labels for each image.

- **Downstream Compatibility:**
  All outputs are directly ingestible by later modules (PhysicsInference, HardNegativeMiner, model training).

#### 5. Example Workflow

```bash
# Generate hard negatives with all advanced strategies
python scripts/generate_hard_negatives.py --input-dir data/raw/ --output data/hard_negatives.txt --parallel 8 --near-miss
```

#### 6. Guarantees

- **Determinism:** All random routines are seeded and logged.
- **Coverage:** Every positive sample yields multiple, diverse, high-quality hard negatives.
- **Audit Trail:** All steps, labels, and mutator chains are logged for review.
- **Scalability:** Easily scales to 600+ hard negatives per phase in minutes.

# Phase-1 Dataset Preparation: Code Structure & Methods for Bongard-Solver

This document outlines a robust, research-backed, and budget-friendly code structure to fully implement the dataset preparation phase in your Bongard-Solver GitHub repository[1]. The design focuses on maximizing label and attribute quality for later modules, with zero monetary outlay, leveraging open-source libraries and NVLabs Bongard-LOGO tools. The structure below can be adopted directly within the existing organization of your repo.

## 1. Repository Directory Layout

```
bongard-solver/
│
├── data/
│   ├── raw/                         # Copy all Bongard-LOGO unmodified files here
│   ├── derived_labels.json          # Generated: physics/geometry/label attributes
│   ├── hard_negatives.txt           # Generated: hard negative/adversarial sample listing
│   ├── flagged_cases.txt            # Output: flagged samples for review (to be generated)
├── scripts/
│   ├── logo_to_shape.py             # Main CLI: parses JSON → computes attributes
│   ├── generate_hard_negatives.py   # CLI: adversarial attribute perturbations
│   ├── verify_annotations.py        # CLI: spot-checks, outputs review queue
│   ├── crowdsource_review.py        # Optional: prepares Zooniverse batches
├── src/
│   ├── data_pipeline/
│   │   ├── __init__.py
│   │   ├── loader.py                # Loader: loads images, categories, built-in splits
│   │   ├── physics_infer.py         # All shape/physics feature extractors (shapely, pymunk, etc.)
│   │   ├── attributes.py            # Categorical label assignment (e.g., Freeform/Basic/Abstract)
│   │   ├── verification.py          # Human-verifiable checks
│   │   └── utils.py
├── tests/
│   ├── test_logo_parser.py
│   ├── test_physics_inference.py
│   ├── test_pipeline.py
│   └── ...
└── README.md
```

## 2. Code Module Details & Interfaces

### 2.1 `src/data_pipeline/loader.py`

- Loads datasets using built-in splits from NVLabs dataset structure (`category_0` and `category_1`)
- Wraps existing `BongardDataset` (from NVLabs) to include derived attribute fields
- Methods:
  - `load_problem(problem_id: str) -> dict`
  - `iter_split(split: str, problem_type: str = None) -> Iterator[dict]`

### 2.2 `src/data_pipeline/logo_parser.py`

- (Deprecated) LOGO parsing logic removed; all geometry and features now loaded from JSON.

### 2.3 `src/data_pipeline/physics_infer.py`

- Accepts vertex arrays, returns numerical/boolean attributes:
  - Centroid (using shapely `Polygon.centroid`)
  - Area, perimeter, moment of inertia (using pymunk, numpy)
  - Convexity (custom: compare hull to vertex list)
  - Symmetry (compare rotated/flipped vertex arrays using root mean square error)
- Batch processing for efficiency

### 2.4 `src/data_pipeline/attributes.py`

- Harvests problem-type and other meta-labels from parent folder and filename conventions
- Appends results to each problem dict for curriculum-style training

### 2.5 `src/data_pipeline/verification.py`

- Receives polygons/labels, flags geometry degeneracies (e.g., self-intersecting polygons)
- Exports flagged cases (under `data/`) for later upload to MakeSense.ai or Zooniverse if needed

### 2.6 `scripts/logo_to_shape.py`

- CLI pipeline interface for batch-processing attribute files:
  - Loads geometry/physics features from JSON
  - Appends meta-labels
  - Saves all outputs as a JSON list under `data/derived_labels.json`
- Arguments: input dir, output file, option to run only on new files

### 2.7 `scripts/generate_hard_negatives.py`

- Enhanced hard-negative mining pipeline integrating:
    - Multi-tier fallback logic (evolutionary, grammar-based, and guaranteed fallback)
    - Shared `Scorer` class for feature extraction and label-flip detection
    - Robust positive label selection (handles all common label variants)
    - Parallel and deterministic execution
    - Comprehensive logging and error handling
  - **Evolutionary search** (`src/hard_negative/evo_search.py`): Uses black-box evolutionary strategies to maximize label flips while preserving geometric validity.
  - **Shape-grammar mutation** (`src/data_pipeline/logo_mutator.py`): Modular rules for stroke-level edits, topology changes, and concept-altering mutations.
  - **Near-miss curriculum**: Optionally stores high-confidence non-flips for progressive classifier retraining.
  - **Parallel execution**: Supports multi-core mining for rapid coverage.
  - **Quality safeguards**: Deterministic seeding, geometry checks, diversity filtering, and audit trail logging.
  - CLI flags: `--parallel`, `--near-miss` for flexible operation.
  - Outputs: `data/hard_negatives.txt` and optionally `data/hard_negatives_nearmiss.txt`.

### 2.8 `scripts/verify_annotations.py`

- Optionally launches browser for MakeSense.ai/manual checks on flagged files

### 2.9 (Optional) `scripts/crowdsource_review.py`

- Prepares image batches for Zooniverse upload when human review is needed
- Handles CSV merge-back after consensus


## 3. Testing & Assurance

- `tests/test_logo_parser.py`: Verify LOGO parsing on canonical/edge-case scripts (unit and integration tests).
- `tests/test_physics_inference.py`: Validate extracted centroid, area, convexity, and symmetry—compare output to known ground-truth for simple shapes.
- `tests/test_pipeline.py`: End-to-end tests: From raw `.logo` to `derived_labels.json`.
- Periodic spot-checks and output reports are automated to flag and detail files that require review, ensuring continuous data quality monitoring.

## 4. Dataflow & Quality Control

1. **Loading**: Start from native Bongard-LOGO structure; never manually relabel classes!
2. **Parsing**: LOGO parser extracts all geometries deterministically.
3. **Attribute & Physics Extraction**: Vectorized batch routines compute centroids, inertia, convexity, symmetry; failures routed for review.
4. **Negative Mining**: For each positive (robustly matched by label), perturbed variants are tested until a “label flip” is found or maximum attempts exhausted. Multi-tier fallback logic guarantees at least one hard negative per positive sample, even for resistant cases.
5. **Lightweight Verification**: Problematic samples auto-flagged. Use MakeSense.ai (or Zooniverse if >20 files needed).
6. **Json Output**: Data for each image includes all computed/derived labels in `data/derived_labels.json`. Hard negatives listed in `data/hard_negatives.txt`.

## 5. Code Best Practices for Data Quality

- **Determinism**: All random routines (including evolutionary search and hard negative mining) are seeded and logged for reproducibility.
- **Batch Processing**: Use `map` or numpy vectorization for geometry/physics routines for speed and consistency.
- **Configurable Thresholds**: For symmetry, convexity, and other features, expose thresholds via pipeline arguments/docs.
- **Fallbacks**: If `.logo` file fails or is corrupted, switch to raster-mode using OpenCV Hu moments.
- **Comprehensive Logging**: Log all flagged or failed geometric computations to a persistent file for later cleanup. All unique label values are logged for each problem to ensure label coverage and auditability.

## 6. Utilization of NVLabs Bongard-LOGO Tools

- Directly use “bongard_logo/dataset.py” for dataset loading and basic utilities.
- Inherit or wrap for your custom field extension (derive further attributes, curriculum flags).
- Leverage their API for any future batch extension (e.g., importing new NVLabs updates).

## 7. Open-Source Libraries to Use

| Purpose                   | Library      | Install Command                |
|---------------------------|-------------|-------------------------------|
| Geometry ops              | shapely     | pip install shapely           |
| Physics metrics           | pymunk      | pip install pymunk            |
| LOGO parsing/turtle sim   | Python std  | (standard; built-in)          |
| Manual spot-check         | MakeSense.ai| (web-based, no install)       |
| Crowdsourcing             | Zooniverse  | (web-based, free registration)|

## 8. Example: Single File Pipeline Usage

```bash
# Run the full derivation pipeline on a subset of 50 puzzles
python scripts/logo_to_shape.py --input-dir data/raw/ --output data/derived_labels.json --problems-list data/phase1_50puzzles.txt


# Generate hard negatives with advanced mining
python scripts/generate_hard_negatives.py --input-dir data/raw/ --output data/hard_negatives.txt --parallel 8 --near-miss

# Spot-check failed samples manually in browser
python scripts/verify_annotations.py --review-list data/flagged_cases.txt
```

## 9. Key Quality Guarantees

- **Determinism:** All random routines (including evolutionary search and hard negative mining) are seeded and logged for reproducibility.
- **Geometry Validity:** All hard negatives pass geometry and self-intersection checks before saving.
- **Diversity:** Embedding-based filtering ensures a diverse set of negatives, avoiding redundancy.
- **Audit Trail:** Each hard negative logs its mutator chain and fitness score for traceability. All label values and sample selection steps are logged for auditability.
- **Scalability:** Parallel mining and curriculum relaxation enable rapid coverage of 600+ hard negatives in under a week.

## 10. Integration with Later Modules


All outputs (especially `derived_labels.json`, `hard_negatives.txt`, and `hard_negatives_nearmiss.txt`) are designed to be directly ingestible by later modules (like `PhysicsInference`, `HardNegativeMiner`, and “alpha” model testing) in your planned Bongard-Solver architecture[1].


**This structure ensures that with open-source libraries, deterministic pipelines, and minimal-but-focused human review, your phase-1 dataset will be robust, high quality, and maximally extensible for future system modules.**

[1] https://github.com/Ayushman125/Bongard-Solver
Updated Report: Phases 0 & 1


Phase 1 Data Requirements and Splits for Bongard-Solver
Categories and Quantities to Annotate for Phase 1

Phase 1 of the Bongard-Solver requires annotated data from all three main Bongard-LOGO categories:
- Freeform Shape Problems
- Basic Shape Problems
- Abstract Shape Problems
For the benchmark and similar research projects, a typical subset for Phase 1 annotation and testing is:
- 50 problems total, distributed as:
  - 17 Freeform Shape problems
  - 17 Basic Shape problems
  - 16 Abstract Shape problems
Within each problem folder, there are 6 positive (category_1) and 6 negative (category_0) images per problem. For each chosen problem, all 12 images (and their associated LOGO scripts) should be annotated with both their inherent labels and the derived attributes (geometry, physics, etc.) for Phase 1 objectives.

**Hard Negative Mining Update:**
- With the new evolutionary and grammar-based pipeline, expected yield is 600–700 hard negatives in 1 week, fulfilling the benchmark for robust downstream training and adversarial evaluation.

Category | No. of Problems | Images per Problem | Total Images to Annotate
---|---|---|---
Freeform | 17 | 12 | 204
Basic | 17 | 12 | 204
Abstract | 16 | 12 | 192
Total | 50 | | 600

Train/Validation/Test Split Ratio
According to the Bongard-LOGO dataset and typical experimental protocols, the standard split used in published work is:
- Training: 70%
- Validation: 15%
- Testing: 15%
For a sample of 50 problems, this results in the following allocation:
- Training: 35 problems
- Validation: 8 problems
- Testing: 7 problems
This ratio ensures a sufficient amount of data for both model learning and reliable performance evaluation, scaled appropriately for your Phase 1 pilot.

Summary Table
Split | Number of Problems | Percentage
---|---|---
Training | 35 | 70%
Validation | 8 | 15%
Testing | 7 | 15%
All selected problems in each split should be fully annotated across the three categories for Phase 1 testing and module validation.


Bongard-Solver Phase-1 Data Processing: Complete Workflow and Command Reference
This step-by-step guide details the full process and command sequence to process, validate, and test Phase-1 data in your current Bongard-Solver repository. The instructions ensure your pipeline is robust and that all data flows are auditable and reproducible.
1. Prerequisites
Install Required Libraries
Open a terminal in your project root and run:
bash
pip install shapely pymunk numpy pillow
•	shapely: Geometry operations
•	pymunk: Physics calculations (moments, inertia)
•	numpy: Numerical processing
•	pillow: Optional, for image handling
2. Data Structure & Initial Checks
Ensure the Directory Tree Is Correct
Your directory should resemble:
text
bongard-solver/
├── data/
│   ├── raw/                 # Unmodified Bongard-LOGO files (Freeform/Basic/Abstract)
│   ├── derived_labels.json  # Output: attributes per image (to be generated)
│   ├── hard_negatives.txt   # Output: hard negatives (optional, to be generated)
│   ├── flagged_cases.txt    # Output: flagged samples for review (to be generated)
├── scripts/
│   ├── logo_to_shape.py
│   ├── generate_hard_negatives.py
│   ├── verify_annotations.py
├── src/
│   ├── data_pipeline/
│   │   ├── loader.py
│   │   ├── logo_parser.py
│   │   ├── physics_infer.py
│   │   ├── attributes.py
│   │   ├── verification.py
│   │   └── utils.py
├── tests/
│   ├── test_logo_parser.py
│   ├── test_physics_inference.py
│   ├── test_pipeline.py
└── README.md
What to Check
•	data/raw/ contains the full Bongard-LOGO data, including images and .logo scripts for each problem and both categories.
3. Run Phase-1 Data Processing Pipeline
Step 1: Generate Per-Image Attribute Labels
Command:
bash
python scripts/logo_to_shape.py --input-dir data/raw/ --output data/derived_labels.json --problems-list data/phase1_50puzzles.txt
•	If you want to process the entire dataset, omit the --problems-list argument.
What to Check After Execution:
•	data/derived_labels.json is created and contains detailed attributes for each image (centroid, area, convexity, symmetry score, moment of inertia, meta-labels, etc.).
•	The script should print or log progress. If you see errors about invalid polygons or failed parsing, these cases will be logged in data/flagged_cases.txt.
Step 2: Generate Hard Negative Samples (If Applicable)
Command:
bash
python scripts/generate_hard_negatives.py --input-dir data/raw/ --output data/hard_negatives.txt
What to Check After Execution:
•	data/hard_negatives.txt is generated and lists the adversarially created samples.
•	If there are any errors or skipped samples, review logs/output for notes.
Step 3: Review Any Flagged/Ambiguous Samples
Command:
bash
python scripts/verify_annotations.py --review-list data/flagged_cases.txt
•	This script lets you manually inspect and resolve problematic files using either a simple viewer or an online tool like MakeSense.ai.
What to Check After Execution:
•	Confirm that flagged_cases.txt is addressed; update/correct any problematic LOGO files or document files to exclude from analysis if unresolvable.
•	Ideally, after fixes, rerun Steps 1–2 to regenerate data as clean as possible.
4. Run Pipeline Tests
Navigate to the project root and execute:
bash
pytest tests/
What to Check After Execution:
•	All unit and integration tests in tests/ should pass.
•	Tests cover: LOGO parsing (all variants), geometry/physics feature extraction, end-to-end attribute pipeline, and (if present) hard negative generation.
5. Inspect and Use Output Files
File	Contents	Next Step/Usage
derived_labels.json	Full annotation per image (labels + computed features)	Input for model training/reasoning
hard_negatives.txt	List of hard negative/adversarial example metadata	Optionally enriches curriculum/training
flagged_cases.txt	Images/scripts flagged for review (invalid shapes, parse fails)	Spot-check and resolve, as above
Typical derived_labels.json Entry Example:
json
{
    "problem_id": "012",
    "image_path": "data/raw/Freeform/012/category_1/img4.png",
    "label": 1,
    "centroid": [101.3, 87.8],
    "area": 4892.1,
    "is_convex": true,
    "symmetry_score": 0.09,
    "moment_of_inertia": 21745.0,
    "problem_type": "Freeform"
}
6. Workflow Recap
1.	Install dependencies
2.	Prepare/verify data/raw/
3.	Run logo_to_shape.py to create derived_labels.json
4.	(Optional) Run generate_hard_negatives.py for adversarial data
5.	Spot-check or crowdsource flagged samples if needed
6.	Run project tests with pytest tests/
7.	Use outputs in downstream tasks, models, or analysis
7. Troubleshooting Tips
•	If output files are empty or missing entries, check script error messages and the format of your data/raw/ directory.
•	For persistent parsing/geometry errors, confirm that all required dependencies are installed and compatible (some older machines may need specific versions of pymunk/shapely).
•	If you add or correct .logo files or resolve flagged_cases.txt, re-run the attribute and negative generation scripts.
Following these steps, your data preparation phase is auditable, systematic, and maximally compatible with all downstream Bongard-Solver modules.




# Integrating ConceptNet Lite into Bongard-Solver Phase 1Before diving into implementation, note the single most important takeaway:

**You cannot run Phase 1 without a local, audited of ConceptNet.**  
That snapshot is the file your code expects at `data/conceptnet_lite.json`, containing every edge you will query. The steps below walk you from zero to a professionally validated JSON dump that satisfies repository contracts, CI schema checks, and licensing obligations.

## 1 Clarifying Project Requirements### 1.1 Why is `conceptnet_lite.json` mandatory?  
`src/commonsense_kb.py` instantiates `CommonsenseKB(path='data/conceptnet_lite.json')`; it immediately loads the file, then bulk-inserts its edges into an SQLite cache. Any absent or malformed file crashes the Phase 1 perception + reasoning pipeline.

### 1.2 Expected schema  
Each entry must be a flat JSON object with four fields:

```json
{
 ```ead": "/c/en/dog",
 ```redicate": "```dFor",
  "```l": "/c/en/pet",
 ```eight": 1.0
}
```

A professional build therefore needs to:

1. Contain **all 34 M edges** from ConceptNet 5.7 (or a documented, reproducible subset).  
2. Preserve Unicode URIs and relation labels exactly as in the source dump.  
3. Pass your JSON-schema validator in `integration/data_validator.py`.

## 2 Overview of the Acquisition PipelineWe rely on the maintained **conceptnet-lite** library, which can (a) download a pre-built SQLite DB or (b) build one from the 21-GB assertions CSV. We choose (a) for speed and then export to JSON.## 3 Step-by-Step Procedure### 3.1 Create a dedicated data folder```bash
mkdir -p```ta/conceptnet_build
```

### 3.2 Install build-time dependencies```bash
# Python```de
pip install```nceptnet-lite tqdm``` System side```nly if you```an to build```om CSV later```udo apt-get install```zip sqlite`````

Conceptnet-lite is licensed Apache 2.0 and bundles all ORM models you need.[1][2][3]

### 3.3 Download the official ConceptNet 5.7 SQLite database```python
import concept```_lite
conceptnet_lite.connect```   "data/conceptnet_build```nceptnet.db"
)           ```downloads ~```B and unpacks automatically````

The call checks for an existing DB, fetches the compressed file from the DigitalOcean CDN, verifies the checksum, unzips, and populates indexes.[2]

*Time cost*: ~10 min on a 200 Mb/s link; disk usage ≈ 9 GB.

> **Alternative (slower)**: pass `db_download_url=None` to trigger a full build from the `conceptnet-assertions-5.7.0.csv.gz` dump provided by the ConceptNet team.[4]

### 3.4 Verify database integrity```bash
sqlite```ata/conceptnet_build/con```tnet.db 'SELECT COUNT``` FROM edges```# Expected```4 074 917
```

If the count differs, re-download—corruption will later surface as missing relations.

### 3.5 Export to the JSON schema required by Bongard-SolverCreate `scripts/export_conceptnet.py`:

```python```port json, sqlite```tqdm, os, sys```RC = "data/conceptnet_build```nceptnet.db"
DST =```ata/conceptnet_lite.json```con = sqlite```onnect(SRC)
cur =```n.cursor()
qry =```"
SELECT  start```rm   AS head```       rel```bel    AS predicate```       end```rm     AS tail```       edges```ight AS weight```OM edges
JOIN relations```S rel  ON edges```lation_id =```l.id
JOIN concepts```AS start ON```ges.start_id =```art.id
JOIN concepts```AS end   ON```ges.end_id  ```end.id;
"""

with open```T, "w", encoding```tf-8") as f```   for row``` tqdm.tqdm(cur.execute```y)):
       ```c = dict(zip(("head", "```dicate", "```l", "weight"), row```        json```mp(rec, f, ensure```cii=False)
       ```write("\n")

con.close``````

Run it:

```bash
python scripts```port_conceptnet.py
```

*Time cost*: 25–30 min on a modern SSD.

### 3.6 Run repository validators```bash
python integration```ta_validator.py --file```ta/conceptnet_lite.json```test tests```st_commonsense_kb.py
```

Both should pass without warnings. The validator checks UTF-8 encoding, required keys, and floating-point weights; the test spins up `CommonsenseKB` and executes sample queries.

### 3.7 Commit and document1. **Do not** add the 6 GB JSON directly to Git history.  
   Instead, create a **GitHub release asset** called `conceptnet_lite.json.zst` (Zstandard compresses to ~1.9 GB).

2. Point the loader to the release URL in `README.md`:

```markdown
wget -q https```github.com//Bong```-Solver/releases/download```.1.0/conceptnet_lite.json```t
unzstd concept```_lite.json.zst -o data```nceptnet_lite.json
```

3. Update CI (`.github/workflows/ci.yml`) to cache the decompressed file.

### 3.8 Licensing and attributionInclude in `NOTICE`:

```
This```oject distributes```derivative```onceptnet_lite.json"
built```om Concept``` 5.7 (CC-BY-SA ```).

Original authors```obyn Speer, Joshua```in, Catherine```vasi.
See http```conceptnet.io and```tps://github.com/commons```e/conceptnet5.

Redistribution```mplies with```ction 4 of```-BY-SA 4.0:
 - Name```e authors
 - Link``` license
 - Ind```te modifications```QLite→JSON, pruning``` assertions`````

## 4 Quality-Control Checklist| Check | Command | Acceptable Range |
|-------|---------|------------------|
| Edge count | `wc -l data/conceptnet_lite.json` | ≥ 34 M |
| UTF-8 validity | `iconv -f utf-8 -t utf-8` | zero errors |
| Average weight | `awk '{sum+=$4} END{print sum/NR}'` | ~ 1.0 ± 0.05 |
| Sample query | `grep '/c/en/dog' | head` | returns `IsA` & `CapableOf` edges |

## 5 Integration with Phase 1 Pipeline1. **First run**: `CommonsenseKB` converts the 6 GB JSON into `data/kb_cache.db` (~1.5 GB, indexed) and stores a pickle of hot queries for fast reload.
2. Subsequent runs detect the cache and skip the initial import, cutting Phase 1 start-up from 3 min to < 10 s.
3. When deploying on CI, invoke:

```bash
python -m src```mmonsense_kb --precache```ta/conceptnet_lite.json````

to bake the SQLite cache into your Docker layer.



## 6 Common Pitfalls & Remedies| Symptom | Likely Cause | Fix |
|---------|--------------|-----|
| `sqlite3.DatabaseError: file is encrypted` | Interrupted download | Delete DB and re-run step 3.3 |
| `json.decoder.JSONDecodeError` during Phase 1 | Newlines stripped by Windows | Ensure you used binary mode when copying; verify with `file -b` |
| CI job exceeds 14 GB RAM on export | Using pandas | Stick to raw SQLite cursor iteration as in 3.5 |
| Edges missing French/Spanish terms | You filtered non-English rows | The Bongard project is language-agnostic; keep multilingual edges |

## 7 Road-Mapped Enhancements1. **Pruned mini-dump**: For low-RAM environments, auto-extract the ~5 M edges whose heads appear in your shape grammar.  
2. **Up-rev to ConceptNet 5.8**: The `conceptnet_lite` maintainer has not yet published a pre-built DB; build from CSV with `db_download_url=None` when 5.8 stabilizes.  
3. **Edge-embedding cache**: If you later move to vector queries, serialize a NumPy memmap of Numberbatch embeddings alongside the JSON.

## 8 ConclusionFollowing the pipeline above yields a **deterministic, auditable, license-compliant `conceptnet_lite.json`** that plugs directly into Bongard-Solver Phase 1. The same procedure underpins professional research codebases and scales to future ConceptNet releases with only minimal tweaks.

Happy reasoning!

**Cited sources**: conceptnet-lite installation and usage, official SQLite dump link, ConceptNet 5.5 paper for attribution.[1][2][4][3][5]



________________________________________
Phase 0: Foundation & System-1 Abstraction Layer (S1-AL)
Objective & Scope
Lay the sensory and orchestration groundwork under MVI-1 constraints. Deliver end-to-end solving on 50 puzzles with performance budgets relaxed ×2 for local hardware. Establish interface contracts, KPI infrastructure, and progressive integration testing to support downstream phases.
Key Enhancements
•	Interface Contract Freeze
• Define and version S1-AL, TaskProfiler, CUDAStreamManager APIs by Week 2.
• Enforce JSON/Protobuf schemas in CI for all new messages.
•	Observability & KPI Dashboard
• Deploy core KPI panels tracking end-to-end solve rate, per-module latencies (P50/P95), and resource utilization.
• Configure automated alerts on latency regressions or drift thresholds.
•	Progressive Integration Tests
• Phase-slice test suite running on a 20-puzzle subset within a 5-minute budget.
• End-to-end validation on 50 puzzles by Week 7, locking all interfaces.
•	Domain-Invariant Feature Extraction
• Physics proxies (COM, inertia tensors, support surfaces).
• DriftMonitor stub for early OOD detection in S1 embeddings.
•	Hybrid Commonsense KB Stub
• ConceptNet-lite loader in src/system1_al.py.
• .query(predicate) hook for social/intention reasoning.
•	Profiling & Scheduling Instrumentation
• integration/task_profiler.py and integration/adaptive_scheduler.py for kernel batching.
• integration/cuda_stream_manager.py for double-buffered host-device transfers.
•	Cross-Module Debug Dashboard
• Single-pane UI correlating S1 outputs, scheduler logs, HIC events, and drift alerts.
Modules & Responsibilities
Module	Responsibility
src/system1_al.py	S1-AL feature extractor with physics proxies and commonsense stub
integration/task_profiler.py	Profiles per-kernel latency and data-transfer overheads
integration/cuda_stream_manager.py	Double-buffered CUDA streams for overlap
integration/adaptive_scheduler.py	Batches GPU/CPU tasks based on profiling insights
integration/hic.py (extended)	Monitors hardware utilization and load-shedding policies
integration/data_validator.py	Enforces JSON/Protobuf schemas in CI
integration/debug_dashboard.py	Real-time correlation of events and performance metrics
src/drift_monitor.py (stub)	Sliding-window drift detection on S1 embeddings
Milestones & Timeline
Week	Deliverable
1–2	Freeze S1-AL, TaskProfiler, CUDAStreamManager interfaces; deploy minimal KPI dashboard
3–4	Integrate AdaptiveScheduler into Resource Orchestrator; enable phase-slice tests (20 puzzles)
5–6	Embed Hybrid Commonsense stub; add DriftMonitor; enforce CI schema and dependency checks
7	Complete unit tests; stabilize end-to-end MVI-1 on 50 puzzles; lock all Phase 0 interfaces
Testing & CI
•	tests/test_system1.py – feature correctness, physics proxies, drift stub
•	tests/test_task_profiler.py – latency logging accuracy
•	tests/test_cuda_stream_manager.py – overlap behavior
•	tests/test_adaptive_scheduler.py – batching decision logic
•	tests/test_hic.py – event routing and load-shedding
•	tests/test_data_validator.py – schema conformance
•	tests/test_debug_dashboard.py – metric correlation integrity
________________________________________
Phase 1: Enhanced Perception & Neural-Symbolic Grounding
Objective & Scope
Complete MVI-1 on the 50-puzzle subset by integrating perception, symbol grounding, dynamic DSL evolution, and initial adversarial mining. Extend the KPI dashboard and integration tests to cover new modules and flows.
Key Enhancements
•	Hybrid Commonsense Fusion
• Load and query ConceptNet-lite in src/physics_inference.py and src/cross_domain_reasoner.py.
• Human-loop bootstrapping feeds new social/intention concepts back into the KB.
•	Dynamic DSL & Meta-Grammar Stub
• src/grammar_extender.py estimates coverage on held-out puzzles; proposes “diff_ratio” operators when < 80%.
• Enforce near-deterministic sampling (τ = 0.3) for template induction, reserving τ > 1.0 for LLM calls.
•	OOD-Aware Embedding Stubs
• Hook in S1-AL for future adversarial domain-classifier integration.
• DriftMonitor alerts on perception embedding shifts.
•	Adversarial & Hard-Negative Mining
• Physics-informed augmentations in data/hard_negative_miner.py.
• CI tests verify diversity metrics and negative injection behavior.
•	Profiling & Scheduling Continuity
• Extend TaskProfiler and AdaptiveScheduler to src/image_augmentor.py and src/physics_inference.py.
• CUDAStreamManager prefetches data for augmentation pipelines.
•	CI-Driven Contracts & Dependency Checks
• Add all Phase 1 modules to integration/data_validator.py.
• Generate dependency-graph reports highlighting new couplings.
Modules & Responsibilities
Module	Responsibility
src/image_augmentor.py (updated)	GPU-batched geometric and relational augmentations
src/physics_inference.py (updated)	Batched COM, stability, affordance calculations; commonsense KB lookups
src/commonsense_kb.py	Loader and embedding-based query API for ConceptNet-lite
src/cross_domain_reasoner.py (updated)	Fuses physics proxies with commonsense predicates
grounder/anytime_inference.py (updated)	Coarse/refine grounding under dynamic time budgets
src/grammar_extender.py (updated)	Meta-Grammar generator stub with deterministic sampling
src/quantifier_module.py	∀/∃ detection on repeated relations
data/hard_negative_miner.py	Physics-informed adversarial sample miner
integration/debug_dashboard.py	Perception and grounding metric panels
integration/data_validator.py	CI-enforced schema validation for new modules
Milestones & Timeline
Week	Deliverable
1–2	Wire TaskProfiler into ImageAugmentor and PhysicsInference; validate CUDAStreamManager overlap
3–4	Embed Hybrid Commonsense KB with human-loop bootstrapping; integrate hard-negative miner
5–6	Implement Meta-Grammar stub; run unsupervised grammar validation on held-out puzzles
7	Achieve end-to-end MVI-1 on 50 puzzles with Phase 1 modules; finalize CI and dependency reports
Testing & CI
•	tests/test_augmentor.py – GPU-batched augmentations and latency profiling
•	tests/test_physics_inference.py – physics proxy accuracy and KB queries
•	tests/test_commonsense_kb.py – predicate query correctness
•	tests/test_cross_domain_reasoner.py – fused reasoning output validation
•	tests/test_anytime_inference.py – grounding under budget constraints
•	tests/test_grammar_extender.py – coverage estimation and stub proposals
•	tests/test_hard_negative_miner.py – adversarial sample diversity metrics
•	tests/test_data_validator.py – schema conformance for Phase 1
•	tests/test_debug_dashboard.py – new panels and metric integrations
________________________________________
This updated report captures both the original Phase 0/1 integrations and the critical success factors—interface contracts, KPI dashboard, and progressive integration tests—needed to support smooth scaling into later phases.


Phase 1: Enhanced Perception & Neural-Symbolic Grounding
Objective & Scope
Deliver MVI-1 end-to-end solving on the 50-puzzle subset by integrating visual perception, symbol grounding, dynamic DSL evolution, and initial adversarial mining. Extend Phase 0’s KPI dashboard, integration tests, and schema validations to cover all new modules and data flows.
Maintain a 2× relaxed performance budget on local hardware (Ryzen 7, 16 GB, RTX 3050 Ti) while ensuring robust CI-driven interface contracts and dependency checks.
________________________________________
Key Enhancements
•	Hybrid Commonsense Fusion
• Load and query ConceptNet-lite in src/physics_inference.py and src/cross_domain_reasoner.py.
• Human-loop bootstrapping feeds new social/intention concepts back into the KB.
•	Dynamic DSL & Meta-Grammar Stub
• src/grammar_extender.py estimates grammar coverage on held-out puzzles; proposes diff_ratio operators when coverage < 80%.
• Enforce near-deterministic sampling (τ = 0.3) for template induction; reserve τ > 1.0 for LLM-backed creative calls.
•	OOD-Aware Embedding Stubs
• Hook System-1 embeddings for future adversarial domain-classifier integration.
• DriftMonitor alerts on perception embedding shifts.
•	Adversarial & Hard-Negative Mining
• Physics-informed augmentations in data/hard_negative_miner.py.
• CI tests to verify diversity metrics and negative sample injection behavior.
•	Profiling & Scheduling Continuity
• Extend TaskProfiler and AdaptiveScheduler to src/image_augmentor.py and src/physics_inference.py.
• CUDAStreamManager prefetches data for the augmentation pipelines.
•	CI-Driven Contracts & Dependency Checks
• Add all Phase 1 modules to integration/data_validator.py.
• Generate automated dependency-graph reports highlighting new couplings.
________________________________________
Modules & Responsibilities
Module	Responsibility
src/image_augmentor.py	GPU-batched geometric and relational augmentations
src/physics_inference.py	Batched COM, stability, affordance calculations; commonsense KB lookups
src/commonsense_kb.py	Loader and embedding-based query API for ConceptNet-lite
src/cross_domain_reasoner.py	Fuses physics proxies with commonsense predicates
grounder/anytime_inference.py	Coarse/refine symbol grounding under dynamic time budgets
src/grammar_extender.py	Meta-Grammar generator stub with deterministic sampling
src/quantifier_module.py	∀/∃ detection on repeated relations
data/hard_negative_miner.py	Physics-informed adversarial sample miner
integration/debug_dashboard.py	Perception and grounding metric panels
integration/data_validator.py	CI-enforced JSON/Protobuf schema validation for all Phase 1 modules
________________________________________
Milestones & Timeline
Week	Deliverable
1–2	Wire TaskProfiler into ImageAugmentor & PhysicsInference; validate CUDAStreamManager prefetch overlap
3–4	Embed Hybrid Commonsense KB with human-loop bootstrapping; integrate hard-negative miner
5–6	Implement Meta-Grammar stub; run unsupervised grammar validation on held-out puzzles
7	Achieve end-to-end MVI-1 on 50 puzzles with Phase 1 modules; finalize CI checks and update dependency reports
________________________________________
Testing & CI
•	tests/test_augmentor.py – GPU-batched augmentations and latency profiling
•	tests/test_physics_inference.py – COM/stability/proxy accuracy and KB queries
•	tests/test_commonsense_kb.py – predicate query correctness
•	tests/test_cross_domain_reasoner.py – fused reasoning output validation
•	tests/test_anytime_inference.py – grounding under budget constraints
•	tests/test_grammar_extender.py – coverage estimation and stub proposals
•	tests/test_hard_negative_miner.py – adversarial diversity sampling metrics
•	tests/test_data_validator.py – schema conformance for Phase 1 modules
•	tests/test_debug_dashboard.py – new dashboard panel integrity
________________________________________
This fully updated Phase 1 document captures the enhanced perception and neural-symbolic grounding integrations, extended CI contracts, and the milestones needed to complete MVI-1 on the 50-puzzle subset. Let me know if you’d like deeper detail on any module or help drafting the Phase 1 smoke test harness!


Phase 2: Enhanced Perception & Neural-Symbolic Grounding – Updated Integrations
Objective & Scope
Deliver end-to-end perception, symbol grounding, and DSL evolution under MVI-1 on the 50-puzzle subset. Budgets are relaxed 2× for local hardware (Ryzen 7, 16 GB, RTX 3050 Ti).
________________________________________
Key Enhancements
•	Profiling & Scheduling
o	TaskProfiler logs per-module latencies and data-transfer overhead.
o	AdaptiveScheduler and CUDAStreamManager overlap GPU/CPU workloads and double-buffer transfers.
•	Hybrid Commonsense KB
o	ConceptNet-lite stub in src/commonsense_kb.py embeds “wants”/“avoids” predicates.
o	Human-AI bootstrapping: expert labels feed back into KB after each HumanLoop cycle.
•	Generative Augmentation
o	GPU-batched rotations, scales, shears, stroke perturbations via AdaptiveScheduler.
o	CI-enforced schema contracts for augmentation parameters.
•	Physics-Augmented Perception
o	Batched COM, support polygon, inertia tensor computation on GPU.
o	“Intent” predicates looked up against the commonsense KB.
•	Hardware-Aware & Progressive Grounding
o	grounder/anytime_inference.py stages coarse (50 ms), refined (150 ms), full (200 ms) passes.
o	Budgets adapt via TaskProfiler; fall back to CPU if PCIe saturates.
•	Dynamic DSL & Meta-Grammar Stub
o	src/grammar_extender.py estimates coverage on held-out puzzles; proposes new operators when < 80%.
o	Deterministic sampling (τ = 0.3) for template induction; high-temperature reserved for creative LLM calls.
o	Unsupervised grammar validation holds out 20% for generalizability; enforces soft limits on grammar size/depth.
•	Adversarial & Hard-Negative Mining
o	Physics-informed augmentations in data/hard_negative_miner.py.
o	CI tests verify diversity metrics and injection into augmentor.
•	CI-Driven Contracts & Dependencies
o	All Phase 2 modules added to integration/data_validator.py.
o	Automated dependency-graph reports flag coupling hotspots.
•	Debug Dashboard Extensions
o	Panels for augmentation latencies, physics metrics, grounding times, grammar events, KB growth.
________________________________________
Modules & Responsibilities
Module	Responsibility
src/image_augmentor.py	GPU-batched geometric & stroke augmentations with profiling
src/physics_inference.py	GPU-accelerated COM, stability, inertia; commonsense KB lookups
src/commonsense_kb.py	ConceptNet-lite loader & embedding-based query
src/cross_domain_reasoner.py	Fuses physics proxies with commonsense predicates
grounder/anytime_inference.py	Hardware-aware coarse/refine/full grounding under time budgets
src/neural_symbolic.py	Confidence-aware neural-symbolic grounding (CI-validated)
src/grammar_extender.py	Dynamic grammar extension & Meta-Grammar stub with coverage checks
src/quantifier_module.py	∀/∃ detection on repeated relations
data/hard_negative_miner.py	Physics-informed hard-negative sample miner
integration/task_profiler.py	Profiles per-task latency & data transfers
integration/cuda_stream_manager.py	Double-buffered host-to-device transfers via CUDA streams
integration/adaptive_scheduler.py	Batches GPU/CPU tasks based on profiling insights
integration/data_validator.py	CI schema enforcement for all Phase 2 modules
integration/debug_dashboard.py	Real-time UI panels for Phase 2 metrics
________________________________________
Milestones & Timeline
Week	Deliverable
1–2	Wire TaskProfiler into augmentor & physics modules; verify CUDAStreamManager overlap
3–4	Embed Hybrid Commonsense KB; enable human-AI bootstrapping; integrate hard-negative miner
5–6	Implement Meta-Grammar stub; perform unsupervised grammar validation on held-out puzzles
7–8	Stabilize MVI-1 slice on 50 puzzles; enforce CI checks & update dependency graphs; extend dashboard
________________________________________
Testing & CI
•	tests/test_augmentor.py — augmentation batching & profiling
•	tests/test_physics_inference.py — GPU physics metrics & KB queries
•	tests/test_commonsense_kb.py — predicate similarity lookups
•	tests/test_cross_domain_reasoner.py — fused reasoning outputs
•	tests/test_anytime_inference.py — grounding budgets & fallbacks
•	tests/test_grammar_extender.py — coverage estimation & stub proposals
•	tests/test_hard_negative_miner.py — adversarial diversity sampling
•	tests/test_data_validator.py — schema conformance for Phase 2
•	tests/test_debug_dashboard.py — integrity of new UI panels
________________________________________
Phase 3: Emergent Codelet Swarm & Hierarchical Schema Induction
Objective & Scope
Deliver a resilient, prioritized codelet swarm and initial schema induction under MVI-2 on 200 puzzles. Budgets relaxed 2× (full codelet cycle ≤ 2 s/puzzle; schema clustering < 60 s).
________________________________________
Key Enhancements
•	Codelet Runtime Modernization
o	AdaptiveScheduler profiles and isolates codelets on CPU threads; GPU used only for heavy embeddings.
o	TimeoutManager enforces per-codelet limits (500 ms) with graceful fallbacks.
o	Thread-pool isolation prevents one codelet stall from blocking the swarm.
•	Dynamic Prioritization via SMA
o	src/meta/sma.py adjusts spawn rates based on puzzle fingerprints and past codelet performance.
o	Distilled policies bias underperforming codelets (e.g., RuleProposer) for targeted exploration.
•	Sampling Temperature Standardization
o	Codelets use τ = 0.2–0.4 for consistency; high τ > 1.0 reserved for creative sub-swarms with downstream validation.
•	Schema Induction & Novelty Hooks
o	src/hame/schema_induction.py uses TaskProfiler-driven scheduling for hierarchical clustering (Ward linkage).
o	Novelty scores from HAME early-exit embeddings trigger Meta-Grammar Generator when abstraction gaps appear.
o	Unsupervised schema validation holds out 10% of puzzles to reject non-generalizable clusters.
•	Debug & Observability
o	Dashboard panels for codelet latencies, spawn distributions, timeout counts, cluster sizes, and novelty histograms.
o	Automated dependency graphs highlight emergent couplings among codelets and schema modules.
•	CI-Driven Contracts & Dependencies
o	Each codelet class must register in CodeletFactory; CI flags missing registrations.
o	Schema JSON outputs validated against integration/data_validator.py.
________________________________________
Modules & Responsibilities
Module	Responsibility
src/codelet_system.py	Adaptive codelet factory, scheduler wrapper, timeout enforcement
src/utils/timeout.py	TimeoutManager for per-codelet time budgets
src/meta/sma.py	StrategicMetacognitiveAgent for spawn-policy adjustments
abstraction/graph_kernel.py	WL graph kernel for scene-graph analogy detection
src/hame/schema_induction.py	SchemaClusterer with novelty estimation and early-exit scheduling
src/grammar_extender.py	Grammar triggers on schema gaps & stub integration
integration/adaptive_scheduler.py	CPU/GPU task batching for codelets
integration/debug_dashboard.py	Panels for codelet and schema induction metrics
integration/data_validator.py	CI schema enforcement for codelet outputs & schemas
________________________________________
Milestones & Timeline
Week	Deliverable
1–2	Scaffold CodeletSystem with AdaptiveScheduler & TimeoutManager; profile using TaskProfiler
3–4	Integrate SMA spawn policies; validate prioritization and sampling temperature regimes
5–6	Implement SchemaClusterer with novelty hooks and early-exit logic; unsupervised schema validation
7–8	Deploy Debug Dashboard extensions; stabilize MVI-2 slice on 200 puzzles under relaxed budgets
________________________________________
Testing & CI
•	tests/test_codelet_system.py — concurrency, isolation, timeout behavior
•	tests/test_timeout_manager.py — per-call deadline enforcement
•	tests/test_sma.py — spawn distribution normalization & policy updates
•	tests/test_graph_kernel.py — WL kernel similarity rankings
•	tests/test_schema_induction.py — cluster counts & novelty scores
•	tests/test_grammar_extender.py — hook firing on simulated gaps
•	tests/test_data_validator.py — schema checks for codelet & schema outputs
•	tests/test_debug_dashboard.py — new codelet/swarm UI panels
________________________________________
Phase 4: Continuous Causal Engine – Updated Integrations
Objective & Scope
Deliver a three-rung causal analysis pipeline under MVI-2 on 200 puzzles with budgets relaxed ×2. Ensure GPU acceleration, adaptive resource use, and early-exit policies to meet strict latency targets.
________________________________________
Key Enhancements
•	Surrogate Graph Pruner
• causal/surrogate_pruner.py fits a lightweight Lasso-based SCM to reduce scene-graph predicates to ≤10 variables.
• Provides a fast approximate causal graph for downstream analysis.
•	Incremental Causal Graph Updater
• causal/incremental_updater.py caches prior graphs and uses KS drift tests to update only changed edges.
• Transforms update cost from O(N³) to near-O(ΔN³) for streaming puzzle data.
•	Adaptive Timeout Estimation
• integration/timeout_estimator.py learns per-module runtime distributions from logs and predicts percentiles.
• Chooses between full PC/GES, surrogate analysis, or skipping counterfactuals when budgets are exceeded.
•	Full Three-Rung Analysis with Graceful Degradation
• Rung 1: GPU-accelerated PC/GES on pruned graphs.
• Rung 2: On-the-fly interventions and ATE estimation.
• Rung 3: DoWhy counterfactual refutation on pruned subgraphs, with early exit on timeout.
•	Metacognitive & Observability Hooks
• integration/debug_dashboard.py panels for prune sizes, GPU/CPU utilization, early-exit counts, and p-value distributions.
• MetacognitiveMonitor logs per-rung latencies, surrogate usage, and ATE variances.
•	CI-Driven Contracts & Schema Validation
• New causal outputs (assoc_matrix, surrogate_used, p_value) validated against versioned JSON/Protobuf schemas in CI.
• Automated dependency graph highlights emerging coupling hotspots in the causal pipeline.
________________________________________
Modules & Responsibilities
Module	Responsibility
causal/surrogate_pruner.py	Prunes scene-graph variables via surrogate Lasso SCM
causal/incremental_updater.py	Caches and incrementally updates causal graphs using drift tests
integration/timeout_estimator.py	Learns and predicts module runtimes; drives early-exit decisions
src/causal_engine.py	Coordinates three-rung analysis with GPU PC/GES, interventions, and counterfactuals
src/utils/timeout.py	Enforces per-call deadlines via UNIX signals
integration/debug_dashboard.py	Provides causal-phase UI panels (prune sizes, exit counts, trends)
________________________________________
Milestones & Timeline
Week	Deliverable
1–2	Implement SurrogatePruner; validate prune accuracy and speed on held-out puzzles
3–4	Build IncrementalUpdater; test ΔN updates on streaming puzzle sequences
5–6	Integrate TimeoutEstimator; configure and test early-exit policies; add CI tests for exit logic
7–8	Extend Debug Dashboard for causal metrics; stabilize MVI-2 slice on 200 puzzles under 200 ms budget
________________________________________
Testing & CI
•	tests/test_surrogate_pruner.py — ranking quality and performance
•	tests/test_incremental_updater.py — drift detection and cache behavior
•	tests/test_timeout_estimator.py — runtime prediction accuracy
•	tests/test_causal_engine.py — end-to-end three-rung outputs and fallback logic
•	tests/test_debug_dashboard.py — causal panels and metric integrity
________________________________________
Phase 5: Abstraction Mining & Concept Library – Updated Integrations
Objective & Scope
Uncover, evaluate, and curate mid-level abstractions—visual, relational, physics, and analogical schemas—under MVI-2 on 200 contexts. Build a self-refining concept library that adapts to drift and supports expert feedback loops.
________________________________________
Key Enhancements
•	Unified Feature Embedding & Fusion
• Stack S1-AL attributes, physics proxies, affordances, elegance scores, and uncertainties.
• GPU-accelerated UMAP reduction to 32-D embeddings for scalable clustering.
•	Hierarchical Schema Clustering with HAME
• abstraction/graph_kernel.py uses Weisfeiler–Lehman graph kernels for scene-graph similarity.
• abstraction/concept_hierarchy.py organizes predicate co-occurrences into multi-level hierarchies.
• abstraction/hame_early_exit.py abandons unlikely analogical candidates when partial distances exceed thresholds.
•	Contrastive Analogy Training
• abstraction/analogy_siamese_gnn.py trains a Siamese GNN on positive/negative Bongard pairs to rank structural analogies.
• Rankings feed into src/abstraction_miner.py for semantically consistent schema proposals.
•	Novelty-Driven Grammar Extensions
• src/grammar_extender.py monitors abstraction coverage gaps (<80%) and proposes new DSL operators.
• Optional LLM-assisted invention, vetted via symbolic checks and expert review.
•	Concept Drift Detection & Adaptation
• src/drift_monitor.py identifies shifts in abstraction embedding distributions over sliding windows.
• On drift, triggers reclustering and resets promotion thresholds.
•	Expert Collaboration & Curation
• src/interactive/human_loop.py and src/meta/expert_collab.py queue high-novelty schemas for human review.
• ECO Feedback Widget captures pairwise judgments to refine utility scoring and grammar proposals.
•	CI-Driven Validation & Boundaries
• Unsupervised hold-out validation (10% contexts) tests cluster generalization before promotion.
• Grammar growth soft-limits enforce periodic consolidation of redundant constructs.
• ConceptLibrary JSON schema validated on each CI build.
________________________________________
Modules & Responsibilities
Module	Responsibility
abstraction/graph_kernel.py	WL graph kernel for scene-graph similarity
abstraction/concept_hierarchy.py	Builds multi-level predicate hierarchies via hierarchical clustering
abstraction/analogy_siamese_gnn.py	Trains and ranks analogies using a Siamese GNN
abstraction/hame_early_exit.py	Early-exit thresholding for partial embedding comparisons
src/drift_monitor.py	Sliding-window drift detection on abstraction embeddings
src/concept_library.py	Loads, queries, and persists concept signatures with cosine lookup
src/abstraction_miner.py	Orchestrates end-to-end extraction, clustering, promotion, and grammar hooks
src/grammar_extender.py	Meta-Grammar Generator and optional LLM hook for new operator proposals
src/interactive/human_loop.py	Expert review queueing and state management
integration/debug_dashboard.py	Abstraction-phase panels: cluster metrics, novelty histograms, ECO queue
________________________________________
Milestones & Timeline
Week	Deliverable
1–2	Implement WLGraphKernel & ConceptHierarchy; validate analogical priors on 100 puzzles
3–4	Train AnalogySiameseGNN on contrastive pairs; integrate EarlyExitHAME for large-scale filtering
5–6	Embed Meta-Grammar Generator and LLM hooks; run unsupervised validation before schema promotion
7–8	Deploy DriftMonitor and ECO review cycles; stabilize MVI-2 slice on 200 contexts with end-to-end pipeline
________________________________________
Testing & CI
•	tests/test_graph_kernel.py — WL kernel similarity and normalization
•	tests/test_concept_hierarchy.py — cluster counts and hierarchies
•	tests/test_analogy_siamese_gnn.py — margin-ranking loss and inference accuracy
•	tests/test_hame_early_exit.py — threshold filtering and timeout behavior
•	tests/test_drift_monitor.py — drift detection and reset logic
•	tests/test_concept_library.py — load, query, and save integrity
•	tests/test_abstraction_miner.py — end-to-end schema extraction and promotion
•	tests/test_grammar_extender.py — Meta-Grammar and LLM hook integration
•	tests/test_human_loop.py — expert queueing and review state transitions
•	tests/test_debug_dashboard.py — abstraction panels and metric synchronization
________________________________________


Phase 6: Nested RL + MCTS Refinement with Elegance, Causality & Grammar Triggers
Objective & Scope
Deliver hierarchical Monte Carlo Tree Search (MCTS) refinement of DSL rules under MVI-3 on a 100-puzzle subset with budgets relaxed ×2. Balance rule accuracy, elegance, causal strength, and grammar complexity through nested search and dynamic grammar evolution.
________________________________________
Key Enhancements
•	Composite State Encoding
• Concatenate embeddings from abstractions, physics margins, causal scores, novelty metrics, and grammar complexity.
• Offload heavy tensor ops via TaskProfiler and AdaptiveScheduler.
•	Elegance-Aware RLMCTS
• Extend UCT with complexity penalty (AST size), prior bias (causal + abstraction weights), and elegance penalty (MEAF cost).
• Train 3-layer MLP policy/value networks on mixed self-play and synthetic puzzles.
•	Nested Rule Composer Actions
• Enable special MCTS actions that merge partial rules into nested ASTs (depth ≤3).
• Enforce per-rollout deadlines (200 ms) with TimeoutManager fallback to shallower search.
•	Dynamic Grammar Extension Triggers
• On low-prior, high-novelty leaf nodes, invoke GrammarExtender for new DSL constructs.
• Re-expand partial trees under updated grammar.
•	Parallel, Budgeted Rollouts
• Use ThreadPoolExecutor (max_workers=4) for concurrent simulations.
• AdaptiveScheduler allocates GPU for policy nets and CPU for AST manipulations.
•	Orchestration & Observability
• HIC publishes per-action metrics; APOE reallocates threads to bottleneck phases.
• MetacognitiveMonitor logs rollout budgets, nested-composer triggers, and grammar events.
• Debug Dashboard panels for rollout counts, average Q-values, grammar triggers, and resource usage.
________________________________________
Modules & Responsibilities
Module	Responsibility
src/rl_mcts.py	EleganceAwareRLMCTS, nested composer, UCT formula extensions
src/utils/timeout.py	TimeoutManager enforcing per-rollout deadlines
src/grammar_extender.py	Grammar triggers on MCTS novelty gaps
integration/adaptive_scheduler.py	GPU/CPU scheduling for policy nets and AST operations
integration/debug_dashboard.py	Panels for rollout metrics, Q-values, grammar events, thread allocations
integration/resource_orchestrator.py	Dynamic CPU/GPU allocation during Phase 6
config/rl_config.yaml	Weights for complexity, prior bias, elegance penalty, and rollout budgets
________________________________________
Milestones & Timeline
Week	Deliverable
1–2	Scaffold StateEncoder and RL_MCTS framework; integrate TaskProfiler and AdaptiveScheduler hooks
3–4	Implement EleganceAwareRLMCTS with complexity/prior-bias terms; begin training policy/value networks
5	Add NestedRuleComposer actions; validate nested AST depth and semantic correctness
6	Wire GrammarExtender triggers on low-prior, high-novelty leaves
7–8	Enable parallel, budgeted rollouts with TimeoutManager; integrate Debug Dashboard panels
9–10	Develop unit tests and ablation studies (nested vs. flat; grammar triggers on/off)
11–12	Performance profiling and final optimizations to meet MVI-3 latency and resource targets
________________________________________
Testing & CI
•	tests/test_rl_mcts.py — policy/value network outputs, UCT complexity/prior penalties
•	tests/test_timeout_manager.py — per-rollout deadline enforcement
•	tests/test_nested_rule_composer.py — nested AST generation depth and validity
•	tests/test_grammar_extender.py — grammar trigger invocation under simulated conditions
•	tests/test_debug_dashboard.py — rollout and grammar panels integrity
•	tests/test_resource_orchestrator.py — Phase 6 thread allocation under load
________________________________________
Phase 7: Meta-Controller & Antifragile Orchestration with Expert Collaboration
Objective & Scope
Implement a master orchestration layer under MVI-3 that dynamically selects and tunes solving strategies per puzzle, fast-adapts key parameters via meta-learning, coordinates expert feedback on ambiguous cases, and triggers antifragile self-improvement routines.
________________________________________
Key Enhancements
•	Fingerprint Extraction
• Build compact puzzle context vectors: shape counts, S1 confidences, causal summaries, novelty scores, grammar complexity, recent metrics.
• TaskProfiler logs extraction latency; HIC-tagged versioning.
•	Neural Contextual Bandit
• src/meta/neural_bandit.py: MLP maps fingerprints to strategy distributions (ILP, RL_MCTS, Codelet, CoT, Hybrid).
• Support exploration/exploitation; CI tests validate probability outputs.
•	MAML Adaptor
• src/meta/maml_adaptor.py: Inner-loop gradient steps on K recent puzzles for hyperparameter tuning.
• Outer-loop meta-parameter snapshots for fast warm-starts with stability regularization.
•	MetaController Core
• src/meta/meta_controller.py:
o	select_strategy(context) → (strategy, fingerprint)
o	dispatch to Orchestrator
o	update_bandit(fp, strategy, reward)
o	adapt_meta(support_data)
• Publishes events to HIC; logs decisions to MetacognitiveMonitor.
•	Orchestrator Dispatcher
• src/meta/orchestrator.py: Routes puzzles to ILPSolver, RL_MCTS, CodeletSystem, CoTReasoner, or HybridSolver.
• Measures solve time and correctness for reward shaping; fallbacks on solver failures.
•	Expert Collaboration Orchestrator (ECO)
• src/meta/expert_collab.py & src/interactive/human_loop.py: Queue high-uncertainty contexts for expert review.
• ECO widget captures judgments to refine GrammarExtender and ConceptLibrary scoring.
•	Self-Improver Routines
• src/meta/self_improver.py: Monitors error rates and triggers targeted retraining (Phase 0 skeletons, Phase 1 augmentors, Phase 4 causal engine, Phase 6 RL nets).
• Logs improvement actions to HIC and Debug Dashboard.
•	Adaptive Performance Optimizer (APOE)
• src/meta/apoe.py: Monitors HIC metrics to reallocate CPU/GPU threads and trigger early-exit heuristics under load.
• Visualizes resource shifts in dashboard.
________________________________________
Modules & Responsibilities
Module	Responsibility
src/fingerprint_extractor.py	Builds and version-tags puzzle fingerprints
src/meta/neural_bandit.py	Bandit network for strategy selection
src/meta/maml_adaptor.py	MAML inner/outer loops for hyperparameter adaptation
src/meta/meta_controller.py	Core logic for strategy selection, bandit updates, and meta-adaptation
src/meta/orchestrator.py	Dispatcher routing puzzles to appropriate solver
src/meta/expert_collab.py	Expert Collaboration Orchestrator and review queueing
src/meta/self_improver.py	Antifragile self-improvement routines
src/meta/apoe.py	Adaptive Performance Optimizer for resource reallocation
integration/hic.py	Hierarchical Integration Controller schema and event-bus extensions
integration/resource_orchestrator.py	Phase-aware CPU/GPU allocation hooks
src/interactive/human_loop.py	Expert review queue and state management
src/concept_library.py	Concept Library load/query/persist logic
integration/debug_dashboard.py	Phase 7 panels: strategy metrics, expert queue status, self-improvements
________________________________________
Milestones & Timeline
Week	Deliverable
1–2	Scaffold FingerprintExtractor & NeuralContextualBandit; validate bandit on synthetic fingerprints
3–4	Integrate MAMLAdaptor for meta-parameter tuning; test inner/outer loops on sandbox puzzles
5	Build MetaController & Orchestrator; verify strategy switching and reward updates
6	Deploy ECO widget; collect first expert-feedback cycles and integrate with GrammarExtender
7	Implement SelfImprover routines for targeted retraining; log actions to HIC and Debug Dashboard
8	Integrate APOE resource optimizer; validate dynamic thread reallocation under heavy load
9	Unit tests: fingerprint extractor, bandit, adaptor, controller, orchestrator, ECO, self-improver, APOE
10	Smoke tests: end-to-end Phase 7 solve
11–12	CI integration, dashboard wiring, docs/phase7.md with architecture diagrams, API specs, and deployment plan
________________________________________
Testing & CI
•	tests/test_fingerprint_extractor.py — fingerprint vector correctness and version tagging
•	tests/test_neural_bandit.py — valid probability distributions and learning dynamics
•	tests/test_maml_adaptor.py — inner/outer loop stability and parameter snapshots
•	tests/test_meta_controller.py — end-to-end strategy selection and update loop
•	tests/test_orchestrator.py — correct routing, fallback behavior, and reward shaping
•	tests/test_expert_collab.py — ECO criteria, queueing, and expert feedback integration
•	tests/test_self_improver.py — retraining triggers and workflows for each phase
•	tests/test_apoe.py — real-time resource reallocation and early-exit enforcement
•	tests/test_debug_dashboard.py — Phase 7 panels and metric synchronization

