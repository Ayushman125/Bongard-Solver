## Concept Registry and Coverage Enforcement
data generation 

A. Central Concept Registry: Self-Maintaining, Auto-Inducing, and Auditable
A new self-maintaining concept registry (`src/concepts/registry.py`) is implemented. This registry automatically induces and caches a concept-checking predicate for every problem in the dataset, using only the features and labels in `derived_labels.json`. No manual wiring or fallback logic is required—coverage is guaranteed for all present and future problems.

#### How It Works
- On first run, the registry loads all problem IDs from `derived_labels.json`.
- For each problem, it extracts the features for positives (`category_1`) and negatives (`category_0`).
- It tries a library of simple, interpretable predicates (e.g., `is_convex`, `num_straight==n`, `has_quadrangle`, etc.) and selects the first that perfectly separates positives from negatives.
- The discovered predicate is cached in `data/concepts_auto.yaml` for auditability and determinism.
- On subsequent runs, the registry loads the cache and uses the stored predicate as the concept function for that problem.
- If new problems appear, the registry induces and caches their rules on the fly, with no crashes or manual edits required.

#### Guarantees
- **Soundness**: Each predicate is chosen only if it separates all supports in the data.
- **Determinism**: The cache file guarantees identical behavior across runs.
- **Auditability**: Each YAML entry records the exact predicate and parameter; reviewers can inspect or override as needed.
- **Performance**: Induction is fast (milliseconds per problem) and negligible at runtime.
- **Extensibility**: Add a new template in one line in `auto_inducer.py` to cover new concept types.

#### Integration
- The hard negative mining pipeline (`scripts/generate_hard_negatives.py`) uses the registry and factory logic. All fallback logic is removed. If a problem is not covered, the process fails fast, ensuring data integrity and full concept coverage.
- The registry is fully compatible with future dataset expansions—new problems are handled automatically.


B. **Hard Negative Mining Pipeline: Modern, Multi-Strategy, and Fully Automated**

#### 1. Multi-Strategy Hard Negative Generation

The hard negative mining pipeline now integrates **five advanced, research-backed strategies** to guarantee both the *quantity* and *quality* of hard negatives for every Bongard-LOGO problem:

- **Deterministic Concept Inversions:**
  For each problem, a registry of per-problem, concept-aware inversion functions is maintained (`src/hard_negative/concept_inversions.py`). These are applied first to generate guaranteed label-flipping negatives for symmetry, arrangement, and other resistant concepts.

- **Metamorphic Affine Inversion Testing (MAIT):**
  A suite of closed-form affine transforms (rotation, scaling, shear, translation) is applied to each positive sample. If the concept function’s output shifts by more than a threshold, the result is recorded as a hard negative. See `src/data_pipeline/affine_transforms.py`.

- **Procedural Shape Perturbation Ensemble (PSPE):**
  Five procedural routines (Perlin jitter, subdivision, wave distortion, radial perturbation, noise scaling) are run in parallel on each sample, each producing a distinct adversarial negative. See `src/data_pipeline/procedural.py`.

- **Geometry-Aware GAN Mining (GAGAN-HNM):**
  A shape-domain GAN is used to generate 1000+ candidate perturbations per sample. The concept function acts as a discriminator, and all label-flipping outputs are retained. See `src/hard_negative/gagan_model.py`.

- **Hyperbolic Hard-Negative Mixup (HHNM):**
  Features are embedded in both Euclidean and hyperbolic space. Top-k hard negatives are mined in each space, and synthetic negatives are created via Poincaré ball mixup. See `src/data_pipeline/hyperbolic.py`.

All strategies are orchestrated in a single, O(1) pipeline in `scripts/generate_hard_negatives.py` (see `process_sample_with_guaranteed_success`). The pipeline is fully parallelizable and deterministic (all random seeds are set via CLI).

#### 2. Deduplication, Geometry Validity, and Diversity

- **Program-Level Deduplication:**
  Each candidate is hashed by its full action program and problem ID to ensure no duplicates are saved.

- **Geometry Validity:**
  All negatives are checked for geometric validity (no self-intersections, minimum vertex count) before being accepted.

- **Diversity Filtering:**
  Embedding-based filtering (L2 distance in feature space) ensures that only diverse negatives are retained, avoiding redundancy.

#### 3. Full Auditability and Coverage

- **Concept Registry:**
  The concept registry (`src/concepts/registry.py`) guarantees that every problem has a valid, auditable concept function. If a new problem appears, the registry auto-induces a separating predicate and caches it for future runs.

- **Logging and Traceability:**
  Every negative logs its mutator chain, fitness score, and the strategy that produced it. All label values and sample selection steps are logged for auditability.

- **Parallel Execution:**
  The pipeline supports multi-core execution for rapid mining of 600–900 hard negatives in minutes.

#### 4. Output and Integration

- **Output Files:**
  - `data/hard_negatives.txt`: List of all generated hard negatives, with full metadata.
  - `data/flagged_cases.txt`: Any samples failing geometry or label checks.
  - `data/derived_labels.json`: All computed features and labels for each image.

- **Downstream Compatibility:**
  All outputs are directly ingestible by later modules (PhysicsInference, HardNegativeMiner, model training).

#### 5. Example Workflow

```bash
# Generate hard negatives with all advanced strategies
python scripts/generate_hard_negatives.py --input-dir data/raw/ --output data/hard_negatives.txt --parallel 8 --near-miss
```

#### 6. Guarantees

- **Determinism:** All random routines are seeded and logged.
- **Coverage:** Every positive sample yields multiple, diverse, high-quality hard negatives.
- **Audit Trail:** All steps, labels, and mutator chains are logged for review.
- **Scalability:** Easily scales to 600+ hard negatives per phase in minutes.

# Dataset Preparation: Code Structure & Methods for Bongard-Solver

This document outlines a robust, research-backed, and budget-friendly code structure to fully implement the dataset preparation phase in your Bongard-Solver GitHub repository[1]. The design focuses on maximizing label and attribute quality for later modules, with zero monetary outlay, leveraging open-source libraries and NVLabs Bongard-LOGO tools. The structure below can be adopted directly within the existing organization of your repo.

## 1. Repository Directory Layout

```
bongard-solver/
│
├── data/
│   ├── raw/                         # Copy all Bongard-LOGO unmodified files here
│   ├── derived_labels.json          # Generated: physics/geometry/label attributes
│   ├── hard_negatives.txt           # Generated: hard negative/adversarial sample listing
│   ├── flagged_cases.txt            # Output: flagged samples for review (to be generated)
├── scripts/
│   ├── logo_to_shape.py             # Main CLI: parses JSON → computes attributes
│   ├── generate_hard_negatives.py   # CLI: adversarial attribute perturbations
│   ├── verify_annotations.py        # CLI: spot-checks, outputs review queue
│   ├── crowdsource_review.py        # Optional: prepares Zooniverse batches
├── src/
│   ├── data_pipeline/
│   │   ├── __init__.py
│   │   ├── loader.py                # Loader: loads images, categories, built-in splits
│   │   ├── physics_infer.py         # All shape/physics feature extractors (shapely, pymunk, etc.)
│   │   ├── attributes.py            # Categorical label assignment (e.g., Freeform/Basic/Abstract)
│   │   ├── verification.py          # Human-verifiable checks
│   │   └── utils.py
├── tests/
│   ├── test_logo_parser.py
│   ├── test_physics_inference.py
│   ├── test_pipeline.py
│   └── ...
└── README.md
```

## 2. Code Module Details & Interfaces

### 2.1 `src/data_pipeline/loader.py`

- Loads datasets using built-in splits from NVLabs dataset structure (`category_0` and `category_1`)
- Wraps existing `BongardDataset` (from NVLabs) to include derived attribute fields
- Methods:
  - `load_problem(problem_id: str) -> dict`
  - `iter_split(split: str, problem_type: str = None) -> Iterator[dict]`

### 2.2 `src/data_pipeline/logo_parser.py`

- (Deprecated) LOGO parsing logic removed; all geometry and features now loaded from JSON.

### 2.3 `src/data_pipeline/physics_infer.py`

- Accepts vertex arrays, returns numerical/boolean attributes:
  - Centroid (using shapely `Polygon.centroid`)
  - Area, perimeter, moment of inertia (using pymunk, numpy)
  - Convexity (custom: compare hull to vertex list)
  - Symmetry (compare rotated/flipped vertex arrays using root mean square error)
- Batch processing for efficiency

### 2.4 `src/data_pipeline/attributes.py`

- Harvests problem-type and other meta-labels from parent folder and filename conventions
- Appends results to each problem dict for curriculum-style training

### 2.5 `src/data_pipeline/verification.py`

- Receives polygons/labels, flags geometry degeneracies (e.g., self-intersecting polygons)
- Exports flagged cases (under `data/`) for later upload to MakeSense.ai or Zooniverse if needed

### 2.6 `scripts/logo_to_shape.py`

- CLI pipeline interface for batch-processing attribute files:
  - Loads geometry/physics features from JSON
  - Appends meta-labels
  - Saves all outputs as a JSON list under `data/derived_labels.json`
- Arguments: input dir, output file, option to run only on new files

### 2.7 `scripts/generate_hard_negatives.py`

- Enhanced hard-negative mining pipeline integrating:
    - Multi-tier fallback logic (evolutionary, grammar-based, and guaranteed fallback)
    - Shared `Scorer` class for feature extraction and label-flip detection
    - Robust positive label selection (handles all common label variants)
    - Parallel and deterministic execution
    - Comprehensive logging and error handling
  - **Evolutionary search** (`src/hard_negative/evo_search.py`): Uses black-box evolutionary strategies to maximize label flips while preserving geometric validity.
  - **Shape-grammar mutation** (`src/data_pipeline/logo_mutator.py`): Modular rules for stroke-level edits, topology changes, and concept-altering mutations.
  - **Near-miss curriculum**: Optionally stores high-confidence non-flips for progressive classifier retraining.
  - **Parallel execution**: Supports multi-core mining for rapid coverage.
  - **Quality safeguards**: Deterministic seeding, geometry checks, diversity filtering, and audit trail logging.
  - CLI flags: `--parallel`, `--near-miss` for flexible operation.
  - Outputs: `data/hard_negatives.txt` and optionally `data/hard_negatives_nearmiss.txt`.

### 2.8 `scripts/verify_annotations.py`

- Optionally launches browser for MakeSense.ai/manual checks on flagged files

### 2.9 (Optional) `scripts/crowdsource_review.py`

- Prepares image batches for Zooniverse upload when human review is needed
- Handles CSV merge-back after consensus


## 3. Testing & Assurance

- `tests/test_logo_parser.py`: Verify LOGO parsing on canonical/edge-case scripts (unit and integration tests).
- `tests/test_physics_inference.py`: Validate extracted centroid, area, convexity, and symmetry—compare output to known ground-truth for simple shapes.
- `tests/test_pipeline.py`: End-to-end tests: From raw `.logo` to `derived_labels.json`.
- Periodic spot-checks and output reports are automated to flag and detail files that require review, ensuring continuous data quality monitoring.

## 4. Dataflow & Quality Control

1. **Loading**: Start from native Bongard-LOGO structure; never manually relabel classes!
2. **Parsing**: LOGO parser extracts all geometries deterministically.
3. **Attribute & Physics Extraction**: Vectorized batch routines compute centroids, inertia, convexity, symmetry; failures routed for review.
4. **Negative Mining**: For each positive (robustly matched by label), perturbed variants are tested until a “label flip” is found or maximum attempts exhausted. Multi-tier fallback logic guarantees at least one hard negative per positive sample, even for resistant cases.
5. **Lightweight Verification**: Problematic samples auto-flagged. Use MakeSense.ai (or Zooniverse if >20 files needed).
6. **Json Output**: Data for each image includes all computed/derived labels in `data/derived_labels.json`. Hard negatives listed in `data/hard_negatives.txt`.

## 5. Code Best Practices for Data Quality

- **Determinism**: All random routines (including evolutionary search and hard negative mining) are seeded and logged for reproducibility.
- **Batch Processing**: Use `map` or numpy vectorization for geometry/physics routines for speed and consistency.
- **Configurable Thresholds**: For symmetry, convexity, and other features, expose thresholds via pipeline arguments/docs.
- **Fallbacks**: If `.logo` file fails or is corrupted, switch to raster-mode using OpenCV Hu moments.
- **Comprehensive Logging**: Log all flagged or failed geometric computations to a persistent file for later cleanup. All unique label values are logged for each problem to ensure label coverage and auditability.

## 6. Utilization of NVLabs Bongard-LOGO Tools

- Directly use “bongard_logo/dataset.py” for dataset loading and basic utilities.
- Inherit or wrap for your custom field extension (derive further attributes, curriculum flags).
- Leverage their API for any future batch extension (e.g., importing new NVLabs updates).

## 7. Open-Source Libraries to Use

| Purpose                   | Library      | Install Command                |
|---------------------------|-------------|-------------------------------|
| Geometry ops              | shapely     | pip install shapely           |
| Physics metrics           | pymunk      | pip install pymunk            |
| LOGO parsing/turtle sim   | Python std  | (standard; built-in)          |
| Manual spot-check         | MakeSense.ai| (web-based, no install)       |
| Crowdsourcing             | Zooniverse  | (web-based, free registration)|

## 8. Example: Single File Pipeline Usage

```bash
# Run the full derivation pipeline on a subset of 50 puzzles
python scripts/logo_to_shape.py --input-dir data/raw/ --output data/derived_labels.json --problems-list data/phase1_50puzzles.txt


# Generate hard negatives with advanced mining
python scripts/generate_hard_negatives.py --input-dir data/raw/ --output data/hard_negatives.txt --parallel 8 --near-miss

# Spot-check failed samples manually in browser
python scripts/verify_annotations.py --review-list data/flagged_cases.txt
```

## 9. Key Quality Guarantees

- **Determinism:** All random routines (including evolutionary search and hard negative mining) are seeded and logged for reproducibility.
- **Geometry Validity:** All hard negatives pass geometry and self-intersection checks before saving.
- **Diversity:** Embedding-based filtering ensures a diverse set of negatives, avoiding redundancy.
- **Audit Trail:** Each hard negative logs its mutator chain and fitness score for traceability. All label values and sample selection steps are logged for auditability.
- **Scalability:** Parallel mining and curriculum relaxation enable rapid coverage of 600+ hard negatives in under a week.

## 10. Integration with Later Modules


All outputs (especially `derived_labels.json`, `hard_negatives.txt`, and `hard_negatives_nearmiss.txt`) are designed to be directly ingestible by later modules (like `PhysicsInference`, `HardNegativeMiner`, and “alpha” model testing) in your planned Bongard-Solver architecture[1].


**This structure ensures that with open-source libraries, deterministic pipelines, and minimal-but-focused human review, your phase-1 dataset will be robust, high quality, and maximally extensible for future system modules.**

[1] https://github.com/Ayushman125/Bongard-Solver
Updated Report: Phases 0 & 1


Phase 1 Data Requirements and Splits for Bongard-Solver
Categories and Quantities to Annotate for Phase 1

Phase 1 of the Bongard-Solver requires annotated data from all three main Bongard-LOGO categories:
- Freeform Shape Problems
- Basic Shape Problems
- Abstract Shape Problems
For the benchmark and similar research projects, a typical subset for Phase 1 annotation and testing is:
- 50 problems total, distributed as:
  - 17 Freeform Shape problems
  - 17 Basic Shape problems
  - 16 Abstract Shape problems
Within each problem folder, there are 6 positive (category_1) and 6 negative (category_0) images per problem. For each chosen problem, all 12 images (and their associated LOGO scripts) should be annotated with both their inherent labels and the derived attributes (geometry, physics, etc.) for Phase 1 objectives.

**Hard Negative Mining Update:**
- With the new evolutionary and grammar-based pipeline, expected yield is 600–700 hard negatives in 1 week, fulfilling the benchmark for robust downstream training and adversarial evaluation.

Category | No. of Problems | Images per Problem | Total Images to Annotate
---|---|---|---
Freeform | 17 | 12 | 204
Basic | 17 | 12 | 204
Abstract | 16 | 12 | 192
Total | 50 | | 600

Train/Validation/Test Split Ratio
According to the Bongard-LOGO dataset and typical experimental protocols, the standard split used in published work is:
- Training: 70%
- Validation: 15%
- Testing: 15%
For a sample of 50 problems, this results in the following allocation:
- Training: 35 problems
- Validation: 8 problems
- Testing: 7 problems
This ratio ensures a sufficient amount of data for both model learning and reliable performance evaluation, scaled appropriately for your Phase 1 pilot.

Summary Table
Split | Number of Problems | Percentage
---|---|---
Training | 35 | 70%
Validation | 8 | 15%
Testing | 7 | 15%
All selected problems in each split should be fully annotated across the three categories for Phase 1 testing and module validation.


Bongard-Solver Phase-1 Data Processing: Complete Workflow and Command Reference
This step-by-step guide details the full process and command sequence to process, validate, and test Phase-1 data in your current Bongard-Solver repository. The instructions ensure your pipeline is robust and that all data flows are auditable and reproducible.
1. Prerequisites
Install Required Libraries
Open a terminal in your project root and run:
bash
pip install shapely pymunk numpy pillow
•	shapely: Geometry operations
•	pymunk: Physics calculations (moments, inertia)
•	numpy: Numerical processing
•	pillow: Optional, for image handling
2. Data Structure & Initial Checks
Ensure the Directory Tree Is Correct
Your directory should resemble:
text
bongard-solver/
├── data/
│   ├── raw/                 # Unmodified Bongard-LOGO files (Freeform/Basic/Abstract)
│   ├── derived_labels.json  # Output: attributes per image (to be generated)
│   ├── hard_negatives.txt   # Output: hard negatives (optional, to be generated)
│   ├── flagged_cases.txt    # Output: flagged samples for review (to be generated)
├── scripts/
│   ├── logo_to_shape.py
│   ├── generate_hard_negatives.py
│   ├── verify_annotations.py
├── src/
│   ├── data_pipeline/
│   │   ├── loader.py
│   │   ├── logo_parser.py
│   │   ├── physics_infer.py
│   │   ├── attributes.py
│   │   ├── verification.py
│   │   └── utils.py
├── tests/
│   ├── test_logo_parser.py
│   ├── test_physics_inference.py
│   ├── test_pipeline.py
└── README.md
What to Check
•	data/raw/ contains the full Bongard-LOGO data, including images and .logo scripts for each problem and both categories.
3. Run Phase-1 Data Processing Pipeline
Step 1: Generate Per-Image Attribute Labels
Command:
bash
python scripts/logo_to_shape.py --input-dir data/raw/ --output data/derived_labels.json --problems-list data/phase1_50puzzles.txt
•	If you want to process the entire dataset, omit the --problems-list argument.
What to Check After Execution:
•	data/derived_labels.json is created and contains detailed attributes for each image (centroid, area, convexity, symmetry score, moment of inertia, meta-labels, etc.).
•	The script should print or log progress. If you see errors about invalid polygons or failed parsing, these cases will be logged in data/flagged_cases.txt.
Step 2: Generate Hard Negative Samples (If Applicable)
Command:
bash
python scripts/generate_hard_negatives.py --input-dir data/raw/ --output data/hard_negatives.txt
What to Check After Execution:
•	data/hard_negatives.txt is generated and lists the adversarially created samples.
•	If there are any errors or skipped samples, review logs/output for notes.
Step 3: Review Any Flagged/Ambiguous Samples
Command:
bash
python scripts/verify_annotations.py --review-list data/flagged_cases.txt
•	This script lets you manually inspect and resolve problematic files using either a simple viewer or an online tool like MakeSense.ai.
What to Check After Execution:
•	Confirm that flagged_cases.txt is addressed; update/correct any problematic LOGO files or document files to exclude from analysis if unresolvable.
•	Ideally, after fixes, rerun Steps 1–2 to regenerate data as clean as possible.
4. Run Pipeline Tests
Navigate to the project root and execute:
bash
pytest tests/
What to Check After Execution:
•	All unit and integration tests in tests/ should pass.
•	Tests cover: LOGO parsing (all variants), geometry/physics feature extraction, end-to-end attribute pipeline, and (if present) hard negative generation.
5. Inspect and Use Output Files
File	Contents	Next Step/Usage
derived_labels.json	Full annotation per image (labels + computed features)	Input for model training/reasoning
hard_negatives.txt	List of hard negative/adversarial example metadata	Optionally enriches curriculum/training
flagged_cases.txt	Images/scripts flagged for review (invalid shapes, parse fails)	Spot-check and resolve, as above
Typical derived_labels.json Entry Example:
json
{
    "problem_id": "012",
    "image_path": "data/raw/Freeform/012/category_1/img4.png",
    "label": 1,
    "centroid": [101.3, 87.8],
    "area": 4892.1,
    "is_convex": true,
    "symmetry_score": 0.09,
    "moment_of_inertia": 21745.0,
    "problem_type": "Freeform"
}
6. Workflow Recap
1.	Install dependencies
2.	Prepare/verify data/raw/
3.	Run logo_to_shape.py to create derived_labels.json
4.	(Optional) Run generate_hard_negatives.py for adversarial data
5.	Spot-check or crowdsource flagged samples if needed
6.	Run project tests with pytest tests/
7.	Use outputs in downstream tasks, models, or analysis
7. Troubleshooting Tips
•	If output files are empty or missing entries, check script error messages and the format of your data/raw/ directory.
•	For persistent parsing/geometry errors, confirm that all required dependencies are installed and compatible (some older machines may need specific versions of pymunk/shapely).
•	If you add or correct .logo files or resolve flagged_cases.txt, re-run the attribute and negative generation scripts.
Following these steps, your data preparation phase is auditable, systematic, and maximally compatible with all downstream Bongard-Solver modules.




# Integrating ConceptNet Lite into Bongard-Solver Phase 1Before diving into implementation, note the single most important takeaway:

**You cannot run Phase 1 without a local, audited of ConceptNet.**  
That snapshot is the file your code expects at `data/conceptnet_lite.json`, containing every edge you will query. The steps below walk you from zero to a professionally validated JSON dump that satisfies repository contracts, CI schema checks, and licensing obligations.

## 1 Clarifying Project Requirements### 1.1 Why is `conceptnet_lite.json` mandatory?  
`src/commonsense_kb.py` instantiates `CommonsenseKB(path='data/conceptnet_lite.json')`; it immediately loads the file, then bulk-inserts its edges into an SQLite cache. Any absent or malformed file crashes the Phase 1 perception + reasoning pipeline.

### 1.2 Expected schema  
Each entry must be a flat JSON object with four fields:

```json
{
 ```ead": "/c/en/dog",
 ```redicate": "```dFor",
  "```l": "/c/en/pet",
 ```eight": 1.0
}
```

A professional build therefore needs to:

1. Contain **all 34 M edges** from ConceptNet 5.7 (or a documented, reproducible subset).  
2. Preserve Unicode URIs and relation labels exactly as in the source dump.  
3. Pass your JSON-schema validator in `integration/data_validator.py`.

## 2 Overview of the Acquisition PipelineWe rely on the maintained **conceptnet-lite** library, which can (a) download a pre-built SQLite DB or (b) build one from the 21-GB assertions CSV. We choose (a) for speed and then export to JSON.## 3 Step-by-Step Procedure### 3.1 Create a dedicated data folder```bash
mkdir -p```ta/conceptnet_build
```

### 3.2 Install build-time dependencies```bash
# Python```de
pip install```nceptnet-lite tqdm``` System side```nly if you```an to build```om CSV later```udo apt-get install```zip sqlite`````

Conceptnet-lite is licensed Apache 2.0 and bundles all ORM models you need.[1][2][3]

### 3.3 Download the official ConceptNet 5.7 SQLite database```python
import concept```_lite
conceptnet_lite.connect```   "data/conceptnet_build```nceptnet.db"
)           ```downloads ~```B and unpacks automatically````

The call checks for an existing DB, fetches the compressed file from the DigitalOcean CDN, verifies the checksum, unzips, and populates indexes.[2]

*Time cost*: ~10 min on a 200 Mb/s link; disk usage ≈ 9 GB.

> **Alternative (slower)**: pass `db_download_url=None` to trigger a full build from the `conceptnet-assertions-5.7.0.csv.gz` dump provided by the ConceptNet team.[4]

### 3.4 Verify database integrity```bash
sqlite```ata/conceptnet_build/con```tnet.db 'SELECT COUNT``` FROM edges```# Expected```4 074 917
```

If the count differs, re-download—corruption will later surface as missing relations.

### 3.5 Export to the JSON schema required by Bongard-SolverCreate `scripts/export_conceptnet.py`:

```python```port json, sqlite```tqdm, os, sys```RC = "data/conceptnet_build```nceptnet.db"
DST =```ata/conceptnet_lite.json```con = sqlite```onnect(SRC)
cur =```n.cursor()
qry =```"
SELECT  start```rm   AS head```       rel```bel    AS predicate```       end```rm     AS tail```       edges```ight AS weight```OM edges
JOIN relations```S rel  ON edges```lation_id =```l.id
JOIN concepts```AS start ON```ges.start_id =```art.id
JOIN concepts```AS end   ON```ges.end_id  ```end.id;
"""

with open```T, "w", encoding```tf-8") as f```   for row``` tqdm.tqdm(cur.execute```y)):
       ```c = dict(zip(("head", "```dicate", "```l", "weight"), row```        json```mp(rec, f, ensure```cii=False)
       ```write("\n")

con.close``````

Run it:

```bash
python scripts```port_conceptnet.py
```

*Time cost*: 25–30 min on a modern SSD.

### 3.6 Run repository validators```bash
python integration```ta_validator.py --file```ta/conceptnet_lite.json```test tests```st_commonsense_kb.py
```

Both should pass without warnings. The validator checks UTF-8 encoding, required keys, and floating-point weights; the test spins up `CommonsenseKB` and executes sample queries.

### 3.7 Commit and document1. **Do not** add the 6 GB JSON directly to Git history.  
   Instead, create a **GitHub release asset** called `conceptnet_lite.json.zst` (Zstandard compresses to ~1.9 GB).

2. Point the loader to the release URL in `README.md`:

```markdown
wget -q https```github.com//Bong```-Solver/releases/download```.1.0/conceptnet_lite.json```t
unzstd concept```_lite.json.zst -o data```nceptnet_lite.json
```

3. Update CI (`.github/workflows/ci.yml`) to cache the decompressed file.

### 3.8 Licensing and attributionInclude in `NOTICE`:

```
This```oject distributes```derivative```onceptnet_lite.json"
built```om Concept``` 5.7 (CC-BY-SA ```).

Original authors```obyn Speer, Joshua```in, Catherine```vasi.
See http```conceptnet.io and```tps://github.com/commons```e/conceptnet5.

Redistribution```mplies with```ction 4 of```-BY-SA 4.0:
 - Name```e authors
 - Link``` license
 - Ind```te modifications```QLite→JSON, pruning``` assertions`````

## 4 Quality-Control Checklist| Check | Command | Acceptable Range |
|-------|---------|------------------|
| Edge count | `wc -l data/conceptnet_lite.json` | ≥ 34 M |
| UTF-8 validity | `iconv -f utf-8 -t utf-8` | zero errors |
| Average weight | `awk '{sum+=$4} END{print sum/NR}'` | ~ 1.0 ± 0.05 |
| Sample query | `grep '/c/en/dog' | head` | returns `IsA` & `CapableOf` edges |

## 5 Integration with Phase 1 Pipeline1. **First run**: `CommonsenseKB` converts the 6 GB JSON into `data/kb_cache.db` (~1.5 GB, indexed) and stores a pickle of hot queries for fast reload.
2. Subsequent runs detect the cache and skip the initial import, cutting Phase 1 start-up from 3 min to < 10 s.
3. When deploying on CI, invoke:

```bash
python -m src```mmonsense_kb --precache```ta/conceptnet_lite.json````

to bake the SQLite cache into your Docker layer.



## 6 Common Pitfalls & Remedies| Symptom | Likely Cause | Fix |
|---------|--------------|-----|
| `sqlite3.DatabaseError: file is encrypted` | Interrupted download | Delete DB and re-run step 3.3 |
| `json.decoder.JSONDecodeError` during Phase 1 | Newlines stripped by Windows | Ensure you used binary mode when copying; verify with `file -b` |
| CI job exceeds 14 GB RAM on export | Using pandas | Stick to raw SQLite cursor iteration as in 3.5 |
| Edges missing French/Spanish terms | You filtered non-English rows | The Bongard project is language-agnostic; keep multilingual edges |

## 7 Road-Mapped Enhancements1. **Pruned mini-dump**: For low-RAM environments, auto-extract the ~5 M edges whose heads appear in your shape grammar.  
2. **Up-rev to ConceptNet 5.8**: The `conceptnet_lite` maintainer has not yet published a pre-built DB; build from CSV with `db_download_url=None` when 5.8 stabilizes.  
3. **Edge-embedding cache**: If you later move to vector queries, serialize a NumPy memmap of Numberbatch embeddings alongside the JSON.

## 8 ConclusionFollowing the pipeline above yields a **deterministic, auditable, license-compliant `conceptnet_lite.json`** that plugs directly into Bongard-Solver Phase 1. The same procedure underpins professional research codebases and scales to future ConceptNet releases with only minimal tweaks.

Happy reasoning!

**Cited sources**: conceptnet-lite installation and usage, official SQLite dump link, ConceptNet 5.5 paper for attribution.[1][2][4][3][5]



________________________________________
Phase 0: Foundation & System-1 Abstraction Layer (S1-AL)
Objective & Scope
Lay the sensory and orchestration groundwork under MVI-1 constraints. Deliver end-to-end solving on 50 puzzles with performance budgets relaxed ×2 for local hardware. Establish interface contracts, KPI infrastructure, and progressive integration testing to support downstream phases.
Key Enhancements
•	Interface Contract Freeze
• Define and version S1-AL, TaskProfiler, CUDAStreamManager APIs by Week 2.
• Enforce JSON/Protobuf schemas in CI for all new messages.
•	Observability & KPI Dashboard
• Deploy core KPI panels tracking end-to-end solve rate, per-module latencies (P50/P95), and resource utilization.
• Configure automated alerts on latency regressions or drift thresholds.
•	Progressive Integration Tests
• Phase-slice test suite running on a 20-puzzle subset within a 5-minute budget.
• End-to-end validation on 50 puzzles by Week 7, locking all interfaces.
•	Domain-Invariant Feature Extraction
• Physics proxies (COM, inertia tensors, support surfaces).
• DriftMonitor stub for early OOD detection in S1 embeddings.
•	Hybrid Commonsense KB Stub
• ConceptNet-lite loader in src/system1_al.py.
• .query(predicate) hook for social/intention reasoning.
•	Profiling & Scheduling Instrumentation
• integration/task_profiler.py and integration/adaptive_scheduler.py for kernel batching.
• integration/cuda_stream_manager.py for double-buffered host-device transfers.
•	Cross-Module Debug Dashboard
• Single-pane UI correlating S1 outputs, scheduler logs, HIC events, and drift alerts.
Modules & Responsibilities
Module	Responsibility
src/system1_al.py	S1-AL feature extractor with physics proxies and commonsense stub
integration/task_profiler.py	Profiles per-kernel latency and data-transfer overheads
integration/cuda_stream_manager.py	Double-buffered CUDA streams for overlap
integration/adaptive_scheduler.py	Batches GPU/CPU tasks based on profiling insights
integration/hic.py (extended)	Monitors hardware utilization and load-shedding policies
integration/data_validator.py	Enforces JSON/Protobuf schemas in CI
integration/debug_dashboard.py	Real-time correlation of events and performance metrics
src/drift_monitor.py (stub)	Sliding-window drift detection on S1 embeddings
Milestones & Timeline
Week	Deliverable
1–2	Freeze S1-AL, TaskProfiler, CUDAStreamManager interfaces; deploy minimal KPI dashboard
3–4	Integrate AdaptiveScheduler into Resource Orchestrator; enable phase-slice tests (20 puzzles)
5–6	Embed Hybrid Commonsense stub; add DriftMonitor; enforce CI schema and dependency checks
7	Complete unit tests; stabilize end-to-end MVI-1 on 50 puzzles; lock all Phase 0 interfaces
Testing & CI
•	tests/test_system1.py – feature correctness, physics proxies, drift stub
•	tests/test_task_profiler.py – latency logging accuracy
•	tests/test_cuda_stream_manager.py – overlap behavior
•	tests/test_adaptive_scheduler.py – batching decision logic
•	tests/test_hic.py – event routing and load-shedding
•	tests/test_data_validator.py – schema conformance
•	tests/test_debug_dashboard.py – metric correlation integrity

________________________________________
Phase 1: Enhanced Perception & Neural-Symbolic Grounding
Objective & Scope
Complete MVI-1 on the 50-puzzle subset by integrating perception, symbol grounding, dynamic DSL evolution, and initial adversarial mining. Extend the KPI dashboard and integration tests to cover new modules and flows.
Key Enhancements
•	Hybrid Commonsense Fusion
• Load and query ConceptNet-lite in src/physics_inference.py and src/cross_domain_reasoner.py.
• Human-loop bootstrapping feeds new social/intention concepts back into the KB.
•	Dynamic DSL & Meta-Grammar Stub
• src/grammar_extender.py estimates coverage on held-out puzzles; proposes “diff_ratio” operators when < 80%.
• Enforce near-deterministic sampling (τ = 0.3) for template induction, reserving τ > 1.0 for LLM calls.
•	OOD-Aware Embedding Stubs
• Hook in S1-AL for future adversarial domain-classifier integration.
• DriftMonitor alerts on perception embedding shifts.
•	Adversarial & Hard-Negative Mining
• Physics-informed augmentations in data/hard_negative_miner.py.
• CI tests verify diversity metrics and negative injection behavior.
•	Profiling & Scheduling Continuity
• Extend TaskProfiler and AdaptiveScheduler to src/image_augmentor.py and src/physics_inference.py.
• CUDAStreamManager prefetches data for augmentation pipelines.
•	CI-Driven Contracts & Dependency Checks
• Add all Phase 1 modules to integration/data_validator.py.
• Generate dependency-graph reports highlighting new couplings.
Modules & Responsibilities
Module	Responsibility
src/image_augmentor.py (updated)	GPU-batched geometric and relational augmentations
src/physics_inference.py (updated)	Batched COM, stability, affordance calculations; commonsense KB lookups
src/commonsense_kb.py	Loader and embedding-based query API for ConceptNet-lite
src/cross_domain_reasoner.py (updated)	Fuses physics proxies with commonsense predicates
grounder/anytime_inference.py (updated)	Coarse/refine grounding under dynamic time budgets
src/grammar_extender.py (updated)	Meta-Grammar generator stub with deterministic sampling
src/quantifier_module.py	∀/∃ detection on repeated relations
data/hard_negative_miner.py	Physics-informed adversarial sample miner
integration/debug_dashboard.py	Perception and grounding metric panels
integration/data_validator.py	CI-enforced schema validation for new modules
Milestones & Timeline
Week	Deliverable
1–2	Wire TaskProfiler into ImageAugmentor and PhysicsInference; validate CUDAStreamManager overlap
3–4	Embed Hybrid Commonsense KB with human-loop bootstrapping; integrate hard-negative miner
5–6	Implement Meta-Grammar stub; run unsupervised grammar validation on held-out puzzles
7	Achieve end-to-end MVI-1 on 50 puzzles with Phase 1 modules; finalize CI and dependency reports
Testing & CI
•	tests/test_augmentor.py – GPU-batched augmentations and latency profiling
•	tests/test_physics_inference.py – physics proxy accuracy and KB queries
•	tests/test_commonsense_kb.py – predicate query correctness
•	tests/test_cross_domain_reasoner.py – fused reasoning output validation
•	tests/test_anytime_inference.py – grounding under budget constraints
•	tests/test_grammar_extender.py – coverage estimation and stub proposals
•	tests/test_hard_negative_miner.py – adversarial sample diversity metrics
•	tests/test_data_validator.py – schema conformance for Phase 1
•	tests/test_debug_dashboard.py – new panels and metric integrations
________________________________________
This updated report captures both the original Phase 0/1 integrations and the critical success factors—interface contracts, KPI dashboard, and progressive integration tests—needed to support smooth scaling into later phases.


Phase 1: Enhanced Perception & Neural-Symbolic Grounding
Objective & Scope
Deliver MVI-1 end-to-end solving on the 50-puzzle subset by integrating visual perception, symbol grounding, dynamic DSL evolution, and initial adversarial mining. Extend Phase 0’s KPI dashboard, integration tests, and schema validations to cover all new modules and data flows.
Maintain a 2× relaxed performance budget on local hardware (Ryzen 7, 16 GB, RTX 3050 Ti) while ensuring robust CI-driven interface contracts and dependency checks.
________________________________________
Key Enhancements
•	Hybrid Commonsense Fusion
• Load and query ConceptNet-lite in src/physics_inference.py and src/cross_domain_reasoner.py.
• Human-loop bootstrapping feeds new social/intention concepts back into the KB.
•	Dynamic DSL & Meta-Grammar Stub
• src/grammar_extender.py estimates grammar coverage on held-out puzzles; proposes diff_ratio operators when coverage < 80%.
• Enforce near-deterministic sampling (τ = 0.3) for template induction; reserve τ > 1.0 for LLM-backed creative calls.
•	OOD-Aware Embedding Stubs
• Hook System-1 embeddings for future adversarial domain-classifier integration.
• DriftMonitor alerts on perception embedding shifts.
•	Adversarial & Hard-Negative Mining
• Physics-informed augmentations in data/hard_negative_miner.py.
• CI tests to verify diversity metrics and negative sample injection behavior.
•	Profiling & Scheduling Continuity
• Extend TaskProfiler and AdaptiveScheduler to src/image_augmentor.py and src/physics_inference.py.
• CUDAStreamManager prefetches data for the augmentation pipelines.
•	CI-Driven Contracts & Dependency Checks
• Add all Phase 1 modules to integration/data_validator.py.
• Generate automated dependency-graph reports highlighting new couplings.
________________________________________
Modules & Responsibilities
Module	Responsibility
src/image_augmentor.py	GPU-batched geometric and relational augmentations
src/physics_inference.py	Batched COM, stability, affordance calculations; commonsense KB lookups
src/commonsense_kb.py	Loader and embedding-based query API for ConceptNet-lite
src/cross_domain_reasoner.py	Fuses physics proxies with commonsense predicates
grounder/anytime_inference.py	Coarse/refine symbol grounding under dynamic time budgets
src/grammar_extender.py	Meta-Grammar generator stub with deterministic sampling
src/quantifier_module.py	∀/∃ detection on repeated relations
data/hard_negative_miner.py	Physics-informed adversarial sample miner
integration/debug_dashboard.py	Perception and grounding metric panels
integration/data_validator.py	CI-enforced JSON/Protobuf schema validation for all Phase 1 modules
________________________________________
Milestones & Timeline
Week	Deliverable
1–2	Wire TaskProfiler into ImageAugmentor & PhysicsInference; validate CUDAStreamManager prefetch overlap
3–4	Embed Hybrid Commonsense KB with human-loop bootstrapping; integrate hard-negative miner
5–6	Implement Meta-Grammar stub; run unsupervised grammar validation on held-out puzzles
7	Achieve end-to-end MVI-1 on 50 puzzles with Phase 1 modules; finalize CI checks and update dependency reports
________________________________________
Testing & CI
•	tests/test_augmentor.py – GPU-batched augmentations and latency profiling
•	tests/test_physics_inference.py – COM/stability/proxy accuracy and KB queries
•	tests/test_commonsense_kb.py – predicate query correctness
•	tests/test_cross_domain_reasoner.py – fused reasoning output validation
•	tests/test_anytime_inference.py – grounding under budget constraints
•	tests/test_grammar_extender.py – coverage estimation and stub proposals
•	tests/test_hard_negative_miner.py – adversarial diversity sampling metrics
•	tests/test_data_validator.py – schema conformance for Phase 1 modules
•	tests/test_debug_dashboard.py – new dashboard panel integrity
________________________________________
This fully updated Phase 1 document captures the enhanced perception and neural-symbolic grounding integrations, extended CI contracts, and the milestones needed to complete MVI-1 on the 50-puzzle subset. Let me know if you’d like deeper detail on any module or help drafting the Phase 1 smoke test harness!



# Phase 1 Roadmap: From Passing Tests to Full Enhanced Perception & Neural-Symbolic Grounding

Bongard-Solver’s Phase 1 objective is to perception, physics proxies, commonsense knowledge, a dynamic mini-grammar, hard-negative mining, and profiling instrumentation so that the system can **solve 50 puzzles end-to-end under the MVI-1 slice**. Although your unit tests green-lit the codebase, the functional pipeline will not run until you (A) generate all Phase 1 artifacts, (B) execute the orchestration scripts in the correct order, (C) confirm schema compliance, and (D) monitor the KPI dashboard for coverage and latency targets. This guide provides a comprehensive, CLI-level walkthrough.

## Overview

Phase 1 transforms the static outputs of Phase 0 (raw images, LOGO scripts, basic physics proxies) into rich **scene graphs plus neural-symbolic feature bundles** ready for grounding. It touches eleven updated/new modules and emits four persistent artifacts:

1. `data/derived_labels.json` – per-image geometry & physics attributes.  
2. `data/augmented.pkl` – GPU-batch augmented images & masks.  
3. `data/scene_graphs.pkl` – fully fused scene graphs with physics proxies.  
4. `data/hard_negatives.txt` – adversarial negatives for curriculum mining.

Skipping any artifact stalls downstream solvers.[1][2]

## Directory Pre-Flight Checklist

Verify the repo tree (top-level extract) shows all required Phase 1 scripts:

| Path | Required? | Purpose |
|---|---|---|
| `scripts/logo_to_shape.py` | YES | Geometry → JSON extraction |
| `scripts/image_augmentor.py` | YES | GPU batched augments |
| `scripts/build_scene_graphs.py` | YES | Constructs scene graphs |
| `scripts/physics_inference.py` | YES | COM, stability, affordances |
| `scripts/generate_hard_negatives.py` | YES | Physics-informed adversarial miner |
| `scripts/export_conceptnet.py` | YES | Dumps ConceptNet-lite JSON |
| `integration/data_validator.py` | YES | Schema enforcement |

If any file is missing, pull from `origin/main` or cherry-pick from prior commits.[3]

## Step-By-Step CLI Execution Flow

### 1. Activate Virtual Environment
```bash
cd Bongard-Solver
.\venv\Scripts\activate            # Windows PowerShell
# OR
source venv/bin/activate           # Unix shells
```
All subsequent paths assume an active venv.

### 2. Install Phase 1 Dependencies
```bash
pip install shapely pymunk cupy-cuda12x cugraph-cuda12x conceptnet-lite==0.3.2 tqdm
```
GPU variants of CuGraph & CuPy accelerate batched physics proxies.[4][5]

### 3. Build ConceptNet-lite Snapshot

ConceptNet edges are mandatory for commonsense fusion.  [1]
```bash
python scripts/export_conceptnet.py \
       --out data/conceptnet_lite.json \
       --languages en,es,de,fr
```
The export consumes ~6 GB disk and ~30 min on SSD.[2]

### 4. Generate Geometry & Physics Attributes
```bash
python scripts/logo_to_shape.py \
       --input-dir data/raw/ \
       --output data/derived_labels.json \
       --parallel 8
```
Expected runtime ≈90 s for 600 images. The script logs per-shape centroid, area, convexity, moment of inertia.[1]

### 5. GPU-Batched Image Augmentation
```bash
python scripts/image_augmentor.py \
       --input data/derived_labels.json \
       --out data/augmented.pkl \
       --parallel 8 \
       --rotate 10 --scale 1.2 --shear 12
```
Operations rely on Augmentor-style transforms. Watch GPU memory (RTX 3050 Ti = 4 GB); default batch size is 64.[4][5]

### 6. Build Scene Graphs
```bash
python scripts/build_scene_graphs.py \
       --aug data/augmented.pkl \
       --out data/scene_graphs.pkl
```
Nodes inherit geometry; edges encode spatial relations (left-of, contains, parallel).[1]

### 7. Inject Physics + Commonsense Proxies
```bash
python scripts/physics_inference.py \
       --scene-graphs data/scene_graphs.pkl \
       --kb data/conceptnet_lite.json \
       --out data/scene_graphs_physics.pkl \
       --gpu
```
Adds COM, stability margins, affordances, and ConceptNet predicates “supports”, “blocks”, “surrounds”.[2]

### 8. Mine Hard Negatives
```bash
python scripts/generate_hard_negatives.py \
       --input-dir data/raw/ \
       --output data/hard_negatives.txt \
       --parallel 8 \
       --near-miss
```
Each positive spawns 12-15 adversarial variants via deterministic transformations.[4]

### 9. Validate Artifacts
```bash
pytest tests/test_data_validator.py::test_phase1_schema -q
```
All JSON/Pickle files must satisfy schema; failures show field-level diffs.

### 10. Run End-to-End MVI-1 Slice
```bash
pytest tests/test_anytime_inference.py::test_mvi1_slice -q
```
This smoke—50 puzzles, full budget—should complete in <6 min on Ryzen 7.[1]

## Profiling & Observability

1. **TaskProfiler**:  
   ```bash
   python -m integration.task_profiler --last-run
   ```
   Inspect P50/P95 latencies; image_augmentor ≤25 ms/it is acceptable.[3]

2. **Debug Dashboard**:  
   Start the panel server:  
   ```bash
   python -m integration.debug_dashboard
   ```
   Watch “Physics Proxy Coverage” (≥95%) and “Input Drift” (≤2 σ).

3. **CUDA Stream Overlap**:  
   Ensure `integration/cuda_stream_manager.py` logged “double-buff OK”.

## Frequently Asked “Why Isn’t It Working?” Checkpoints

| Symptom | Root Cause | Fix |
|---|---|---|
| `CommonsenseKB: file not found` | Missing `conceptnet_lite.json` | Run **Step 3** export again[2] |
| `CUDA_ERROR_OUT_OF_MEMORY` | Batch too large | Set `--batch 32` on augmentor[4] |
| `Schema mismatch: field 'convexity' absent` | Old cache of `derived_labels.json` | Delete file, re-run **Step 4** |
| `Augmentor KeyError: shear` | CuPy build missing SciPy ops | `pip install cupyx-scikit-image`[5] |

## Automation Script

Add `scripts/run_phase1.ps1`:
```powershell
.\venv\Scripts\activate
python scripts/export_conceptnet.py --out data/conceptnet_lite.json
python scripts/logo_to_shape.py --input-dir data/raw/ --output data/derived_labels.json --parallel 8
python scripts/image_augmentor.py --input data/derived_labels.json --out data/augmented.pkl --parallel 8
python scripts/build_scene_graphs.py --aug data/augmented.pkl --out data/scene_graphs.pkl
python scripts/physics_inference.py --scene-graphs data/scene_graphs.pkl --kb data/conceptnet_lite.json --out data/scene_graphs_physics.pkl --gpu
python scripts/generate_hard_negatives.py --input-dir data/raw/ --output data/hard_negatives.txt --parallel 8 --near-miss
pytest tests/test_anytime_inference.py::test_mvi1_slice -q
```
Make executable and version-control for reproducibility.

## What NOT to Re-Run

- **ConceptNet Export**: Only repeat if you delete the JSON or upgrade to a newer ConceptNet snapshot.  [2]
- **Hard-Negative Mining**: Skip until you add fresh raw puzzles or tweak physics thresholds; the file is deterministic.  
- **Image Augmentation**: Re-run only if you change rotation/shear parameters; checksum caching avoids redundant work.

## KPIs to Declare Phase 1 “Done”

| KPI | Target | Where to View |
|---|---|---|
| Puzzle coverage (50/50) | 100% | Dashboard “Coverage” |
| Augment throughput | ≥150 img/s | TaskProfiler |
| Drift alert count | 0 | Dashboard “S1 Drift” |
| Avg grounding latency | ≤200 ms | Smoke test log |
| Schema violations | 0 | DataValidator |

Meeting these thresholds means Phase 1’s Enhanced Perception & Neural-Symbolic Grounding layer is production-ready and you can branch into Phase 2 development.

### Key Takeaways

- **Sequence matters**: derived → augment → graph → physics → negative.  
- **ConceptNet is non-optional**: no KB, no commonsense fusion.  [1][2]
- **CI is your gatekeeper**: rely on `tests/test_data_validator.py` and `test_mvi1_slice` before merging to `main`.

Proceed with confidence—your foundation for later causal reasoning and codelet swarms now stands on solid, reproducible Phase 1 artifacts.





# Phase 2: Enhanced Perception & Neural-Symbolic Grounding – Detailed Implementation Guide  

**Objective & Scope**  
Complete Phase 2 of the Bongard-Solver on local hardware (zen 7, 16 GB RAM, RTX 3050 Ti) by integrating:  
1. A hybrid perception module that fuses lightweight cues, Tiny-CLIP embeddings, and ConceptNet-Lite lookups.  
2. A Multimodal Chain-of-Thought (MCoT) wrapper for transparent LLM-assisted inference.  
3. Symbol grounding via anonymous shape IDs and cue-based clustering.  
4. A slim DSL (“small-to-large” ladder) with five primitive predicates and dynamic coverage checks.  
5. Hard-negative mining continuations, profiling instrumentation, and CI contracts.  

Maintain 2× relaxed latency budgets (P50 ≤ 200 ms for perception, P95 ≤ 750 ms for grounding), enforce schema validations, and extend the dashboard to surfacing Phase 2 KPIs.  

## 1. Repository & Folder Structure  

```
bongard-solver/
├── data/
│   ├── raw/                           # Phase 1 raw inputs
│   ├── derived_full.csv               # 600×52 primitive-cue table
│   ├── conceptnet_lite.json           # downloaded Phase 1 KB
│   ├── scene_graphs_physics.pkl.zst   # Phase 1 scene graphs + physics
│   └── hard_negatives.txt             # Phase 1 negatives
├── src/
│   ├── cues/
│   │   └── primitive_cues.py          # stroke count, Hu moments, Zernike, symmetry
│   ├── perception/
│   │   ├── encoder.py                 # Tiny-CLIP + cue fusion
│   │   ├── cot_wrapper.py             # MCoT prompt & parse logic
│   │   └── config.yaml                # batch sizes, thresholds
│   ├── grounding/
│   │   ├── shape_aligner.py           # cosine clustering (θ=0.82)
│   │   ├── dsl/
│   │   │   ├── primitives.py          # larger(x,y), inside(x,y), sym(x,a), stroke_count, touch
│   │   │   └── coverage_checker.py    # dynamic DSL “ladder” logic
│   │   └── codelets.py                # Propose_Cue_Rule, Merge_Shapes, Try_Quantifier, Search_Spatial_Pair, Prune
│   ├── profiling/
│   │   ├── task_profiler.py           # per-module P50/P95 logging
│   │   └── cuda_stream_manager.py     # double-buffered transfers
│   └── utils/
│       └── schema_validator.py        # CI-driven JSON/Protobuf checks
├── scripts/
│   ├── extract_cues.py                # build derived_full.csv from derived_labels.json
│   ├── encode_embeddings.py           # run encoder.py in batch
│   ├── run_cot.py                     # wrap llava inference on fused embeddings
│   ├── ground_rules.py                # orchestrate shape alignment & codelet swarm
│   └── validate_phase2.py             # end-to-end Phase 2 smoke test
├── tests/
│   ├── test_primitive_cues.py
│   ├── test_encoder.py
│   ├── test_cot_wrapper.py
│   ├── test_shape_aligner.py
│   ├── test_primitives.py
│   ├── test_codelets.py
│   ├── test_task_profiler.py
│   └── test_schema_validator.py
└── integration/
    └── debug_dashboard.py             # panels: cue coverage, embed drift, grounding latency
```

## 2. Step-by-Step Implementation  

### 2.1 Primitive Cue Extraction  
1. **File:** `src/cues/primitive_cues.py`  
2. **Implement:**  
   - `compute_stroke_count(polygon)`  
   - `total_stroke_length(polygon)`  
   - `mean_curvature(polygon)`  
   - `hu_moments(polygon)` (first 3)  
   - `zernike_moments(polygon, order=4)`  
   - `axis_symmetry_score(polygon)` via 2D FFT  
3. **Script:**  
   ```bash
   python scripts/extract_cues.py \
       --input data/derived_labels.json \
       --output data/derived_full.csv
   ```
4. **Validate:**  
   - CSV has 600 rows × 52 fields.  
   - Cue distributions near‐uniform (check in dashboard).  

### 2.2 Fused Embedding Encoder  
1. **File:** `src/perception/encoder.py`  
   - Load Tiny-CLIP ViT-B/16 (batch=8).  
   - Read per‐image cue vector (size = 52).  
   - Concatenate to 512 + 52 = 564 d, then L2‐normalize.  
2. **Script:**  
   ```bash
   python scripts/encode_embeddings.py \
       --cues data/derived_full.csv \
       --out data/embeddings.npy \
       --batch 8
   ```
3. **Test:**  
   - `pytest tests/test_encoder.py`  
   - Embedding dims & norms correct.  

### 2.3 Multimodal Chain-of-Thought (MCoT) Wrapper  
1. **File:** `src/perception/cot_wrapper.py`  
   - Template with prompt:  
     ```
     You are a visual analyst.
     Q: {query}
     IMG_EMBED: {embed}
     Think step by step in two lines:
     1. Primitive cues noticed.
     2. Short logic clause.
     A:
     ```
2. **LLM Backend:** Llava-1.5-7B-Q4 via `llama.cpp --n-gpu-layers 20`.  
3. **Script:**  
   ```bash
   python scripts/run_cot.py \
       --embeddings data/embeddings.npy \
       --queries data/queries.jsonl \
       --out data/cot_outputs.jsonl
   ```
4. **Test:**  
   - `pytest tests/test_cot_wrapper.py` for parse correctness.  

### 2.4 Symbol Grounding & Rule Induction  
#### 2.4.1 Anonymous Shape IDs & Clustering  
1. **File:** `src/grounding/shape_aligner.py`  
   - Assign `shape_{id}` to each connected component.  
   - Compute cosine similarity on fused embeddings.  
   - Single‐link clustering with threshold = 0.82.  

#### 2.4.2 DSL Primitives & Coverage Ladder  
1. **File:** `src/grounding/dsl/primitives.py`  
   - Define five functions: `larger(x,y)`, `inside(x,y)`, `sym(x,axis)`, `stroke_count(x,n)`, `touch(x,y)`.  
2. **Coverage Checker:** `coverage_checker.py` determines percentage of puzzles solvable by primitives alone.  
3. **Graduation Logic:** When coverage < 80%, trigger dynamic grammar extender (Phase 3 stub).  

#### 2.4.3 Codelet Swarm  
1. **File:** `src/grounding/codelets.py`  
   - Five thread‐safe codelets:  
     - `ProposeCueRule`  
     - `MergeShapes`  
     - `TryQuantifier`  
     - `SearchSpatialPair`  
     - `Prune`  
   - Each must complete < 40 ms (watchdog).  
   - Append CoT rationale to a shared trace store.  
2. **Orchestration Script:**  
   ```bash
   python scripts/ground_rules.py \
       --cot data/cot_outputs.jsonl \
       --embeddings data/embeddings.npy \
       --out data/phase2_rules.json
   ```
3. **Test:**  
   - `pytest tests/test_shape_aligner.py`  
   - `pytest tests/test_primitives.py`  
   - `pytest tests/test_codelets.py`  

### 2.5 Hard-Negative Mining Continuation  
- Continue Phase 1’s progressive miner: `k=5` per puzzle per epoch.  
- Ensure new negatives integrate into `hard_negatives.txt`.  

### 2.6 Profiling & CI Contracts  
1. **Profiling:**  
   - Integrate `task_profiler.py` to log P50/P95 for encoder, CoT, grounding.  
   - Dashboard panels in `integration/debug_dashboard.py`:  
     - **Cue Extraction Coverage**  
     - **Embedding Drift**  
     - **Grounding Latencies**  
2. **Schema Validation:**  
   - Define JSON schemas for:  
     - `derived_full.csv` fields  
     - `embeddings.npy` shape metadata  
     - `cot_outputs.jsonl` fields  
     - `phase2_rules.json` rule format  
   - Run `scripts/validate_phase2.py` in CI.  
3. **Tests:**  
   ```bash
   pytest tests/test_task_profiler.py
   pytest tests/test_schema_validator.py
   ```

## 3. Phase 2 Timeline (2 Months)  

| Week | Deliverable                                 | Validation                         |
|------|---------------------------------------------|------------------------------------|
| 1    | Implement & test primitive cues extraction  | test_primitive_cues.py green        |
| 2    | Build fused encoder & embedding script      | test_encoder.py; P50 ≤ 50 ms       |
| 3    | Develop MCoT wrapper & run end-to-end       | test_cot_wrapper.py; drift ≤ 2 σ   |
| 4    | Shape alignment & primitive DSL             | tests for primitives & aligner       |
| 5    | Codelet swarm orchestration & rule output   | test_codelets.py; latency < 40 ms  |
| 6    | Integrate hard-negative updates             | review hard_negatives.txt          |
| 7    | Profiling integration & dashboard panels    | inspect debug_dashboard            |
| 8    | CI schema validation & full smoke test      | scripts/validate_phase2.py; tests pass |

## 4. Key KPIs & Risk Mitigation  

| Risk                       | KPI Target               | Mitigation                                 |
|----------------------------|--------------------------|--------------------------------------------|
| Cue extraction failure     | 600/600 rows extracted   | Unit tests; fallback NumPy implementation  |
| Embedding OOM              | GPU mem < 3.8 GB         | batch=8; split on CPU if necessary        |
| LLM timeout                | CoT < 200 ms/query       | Reduce prompt size; cache partial chains   |
| Grounding latency drift    | P95 < 750 ms             | Codelet timeouts; profiling alerts         |
| Schema violations          | 0                        | CI-driven validator; strict pre-commit     |

**Phase 2 Completion** is achieved when:  
- All tests in `tests/` pass.  
- `scripts/validate_phase2.py` reports zero schema errors.  
- Debug dashboard shows cue coverage ≥ 93%, embedding drift ≤ 2 σ, grounding latencies within targets.  
- 100% of the 50-puzzle MVI-1 slice is solved end-to-end under Phase 2 modules.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/6010715/f85bb9ce-0f1d-4083-86a7-d6a974339adb/projectreport.txt
[2] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/6010715/a758f4d2-5f83-45ed-9da1-f66cd767bb12/A-Hybrid-AI-Architecture-for-Solving-Bongard-Problems.docx


# Phase 3: Emergent Codelet Swarm & Hierarchical Schema Induction – Detailed Implementation Guide  

**Objective & Scope**  
Extend the Bongard-Solver from Phase 2’s grounding DSL bootstrapping into a resilient, prioritized **codelet swarm** and initial **schema induction** under local constraints (Ryzen 7, 16 GB, RTX 3050 Ti). Deliver MVI-2 slice performance on a 200-puzzle subset with a relaxed 2× latency budget (full codelet cycle ≤ 2 s, schema clustering "`.  
- Return `(predicate, why)`.

**Tests:**  
```bash
pytest tests/test_codelet_factory.py
pytest tests/test_timeout_manager.py
```

### 2.4 Strategic Metacognitive Agent (SMA)  
1. **File:** `src/meta/sma.py`  
2. **Implement:**  
   - Load fingerprints CSV.  
   - For each puzzle, compute spawn weights per codelet using softmax over negative past latency + error rates.  
   - Output per-puzzle schedule (e.g., 10 runs of ProposeCueRule, 5 of MergeShapes).  
3. **Integration:** Called by swarm orchestrator before each puzzle.  
4. **Test:**  
   ```bash
   pytest tests/test_sma.py
   ```

### 2.5 Swarm Orchestration Script  
1. **File:** `scripts/run_codelet_swarm.py`  
2. **Implement:**  
   - For each puzzle:  
     1. Load context fingerprint and scene graphs.  
     2. Invoke SMA for schedule.  
     3. Spawn codelets via `AdaptiveScheduler`, gather proposals and `why`.  
     4. Aggregate proposals: rank by confidence (e.g., rule coverage) and prune low-scoring ones.  
     5. Log trace to `phase3_swarm_traces.jsonl`.  
3. **Command:**  
   ```bash
   python scripts/run_codelet_swarm.py \
     --puzzles data/phase2_scene_graphs_physics.pkl \
     --fingers data/phase3_fingerprints.csv \
     --out data/phase3_swarm_traces.jsonl
   ```
4. **Smoke Test:**  
   ```bash
   pytest tests/test_codelet_factory.py  # integration test included
   ```

### 2.6 Hierarchical Schema Induction  
1. **File:** `src/abstraction/graph_kernel.py`  
   - Compute WL kernel embeddings for each puzzle’s scene graph.  
2. **File:** `src/abstraction/schema_induction.py`  
   - Perform agglomerative clustering (Ward linkage) on embedded scene graphs.  
   - Compute **novelty scores**: distance to nearest existing cluster center.  
   - Retain clusters up to 10 levels; output JSON detailing cluster IDs and member puzzles.  
3. **Script:**  
   ```bash
   python scripts/induce_schemas.py \
     --embeddings data/phase2_scene_graphs_physics.pkl \
     --out data/phase3_schema_clusters.json
   ```
4. **Test:**  
   ```bash
   pytest tests/test_graph_kernel.py
   pytest tests/test_schema_induction.py
   ```

## 3. Phase 3 Timeline (2 Months)  

| Week | Deliverable                                       | Validation                               |
|------|---------------------------------------------------|------------------------------------------|
| 1    | Fingerprint extractor & SMA prototype             | test_sma.py green; sample schedules     |
| 2    | TimeoutManager & Codelet Base/Factory integration | test_timeout_manager.py; factory coverage |
| 3    | Implement five codelets & register                | tests for each codelet; latency <500 ms |
| 4    | AdaptiveScheduler integration & profiling         | profiling.py logs; dashboard panel      |
| 5    | Swarm orchestration script end-to-end             | runs on 50 puzzles; traces logged       |
| 6    | WL Graph Kernel embeddings                        | test_graph_kernel.py; embedding dims    |
| 7    | Schema induction & novelty scoring                | clusters JSON; test_schema_induction.py  |
| 8    | Dashboard panels & CI schema validation           | scripts/validate_phase3.py passes; dashboard |

## 4. Key KPIs & Risk Mitigation  

| Risk                         | KPI Target                      | Mitigation                                 |
|------------------------------|---------------------------------|--------------------------------------------|
| Codelet hang or overrun      | 0 timeouts per 200 puzzles      | TimeoutManager; CI test timeouts          |
| Unbalanced spawn distribution| entropy ≥ 1.5 bits per puzzle   | SMA softmax temperature tuning            |
| Low proposal quality         | average rule F1 ≥ 0.65          | prune codelet; adjust quotas              |
| Schema overfitting           | hold-out cluster purity ≥ 0.7   | unsupervised validation; novelty pruning  |
| Trace log errors             | 0 malformed JSONL entries       | schema validator in CI; pre-commit hook   |

**Phase 3 Completion** is achieved when end-to-end MVI-2 on 200 puzzles:  
- All codelets run within deadlines, producing rule proposals that achieve ≥ 65% average F1.  
- Schema induction clusters validated on a held-out 20% subset with purity and novelty within thresholds.  
- CI schema checks pass and the debug dashboard reflects stable metrics.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/6010715/f85bb9ce-0f1d-4083-86a7-d6a974339adb/projectreport.txt
[2] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/6010715/a758f4d2-5f83-45ed-9da1-f66cd767bb12/A-Hybrid-AI-Architecture-for-Solving-Bongard-Problems.docx




# Phase 4: Continuous Causal Engine – Detailed Implementation Guide  

**Objective & Scope**  
Enhance the Bongard-Solver with a three-rung causal‐analysis pipeline (PC‐→Intervention→Counterfactual) on local hardware (Ryzen 7, 16 GB RAM, RTX 3050 Ti), meeting MVI-2 targets on a 200-puzzle slice. Enforce strict latency budgets (200 ms per prune, 750 ms per intervention, 2 s per full analysis) and adaptively degrade when necessary.

## 1. Repository & Folder Structure  

```
bongard-solver/
├── data/
│   └── phase4_causal_inputs.pkl          # fused cue+scene embeddings from Phase 3
├── src/
│   ├── causal/
│   │   ├── surrogate_pruner.py           # Lasso‐based variable pruning
│   │   ├── incremental_updater.py        # drift‐aware graph update
│   │   └── do_why_interface.py           # wrappers for DoWhy counterfactuals
│   ├── timeout/
│   │   └── timeout_manager.py            # per‐call deadline enforcement
│   └── causal_engine.py                  # three‐rung orchestrator
├── scripts/
│   ├── run_surrogate_pruner.py           # prune to ≤12 vars per puzzle
│   ├── update_causal_graphs.py           # incremental graph maintenance
│   ├── run_interventions.py              # ATE estimation on pruned graph
│   └── run_counterfactuals.py            # DoWhy refutations with early exit
├── tests/
│   ├── test_surrogate_pruner.py
│   ├── test_incremental_updater.py
│   ├── test_timeout_manager.py
│   ├── test_causal_engine.py
│   └── test_do_why_interface.py
└── integration/
    └── debug_dashboard.py                # panels: prune sizes, p-value distributions, exit counts
```

## 2. Step-by-Step Implementation  

### 2.1 Surrogate Pruner (Rung 1)  
1. **File:** `src/causal/surrogate_pruner.py`  
2. **Implement:**  
   - Load per-puzzle variables (cue features ≤ 52 + scene graph props).  
   - Fit a Lasso regression (`α=0.01`) to predict label membership.  
   - Select top 12 covariates with nonzero coefficients.  
3. **Script:**  
   ```bash
   python scripts/run_surrogate_pruner.py \
     --input data/phase4_causal_inputs.pkl \
     --output data/phase4_pruned_vars.pkl
   ```
4. **Test:**  
   ```bash
   pytest tests/test_surrogate_pruner.py
   ```
5. **KPI:** Prune time P95 ≤ 200 ms; variables ≤ 12.

### 2.2 Incremental Graph Updater (Rung 1.5)  
1. **File:** `src/causal/incremental_updater.py`  
2. **Implement:**  
   - Load previous graph edges and new pruned variables.  
   - Apply Kolmogorov–Smirnov drift test per edge; update only changed relations.  
3. **Script:**  
   ```bash
   python scripts/update_causal_graphs.py \
     --old data/prev_graph.pkl \
     --pruned data/phase4_pruned_vars.pkl \
     --out data/phase4_graph.pkl
   ```
4. **Test:**  
   ```bash
   pytest tests/test_incremental_updater.py
   ```
5. **KPI:** Update time P95 ≤ 150 ms.

### 2.3 Average Treatment Effect Estimation (Rung 2)  
1. **File:** `src/causal/causal_engine.py` (method `estimate_ate`)  
2. **Implement:**  
   - For each candidate predicate, perform back-door adjustment via `do_why_interface`.  
   - Estimate ATE with 50 bootstrap samples; record p-values.  
3. **Script:**  
   ```bash
   python scripts/run_interventions.py \
     --graph data/phase4_graph.pkl \
     --out data/phase4_ate_results.json
   ```
4. **Test:**  
   ```bash
   pytest tests/test_causal_engine.py::test_ate_estimation
   ```
5. **KPI:** Intervention time P95 ≤ 750 ms; bootstrap p > 0.05 pruned.

### 2.4 Counterfactual Refutation (Rung 3)  
1. **File:** `src/causal/do_why_interface.py`  
2. **Implement:**  
   - Wrap DoWhy’s API with a timeout guard (200 ms) via `timeout_manager`.  
   - For each high-confidence edge, compute counterfactual query; capture effect size.  
3. **Script:**  
   ```bash
   python scripts/run_counterfactuals.py \
     --ate data/phase4_ate_results.json \
     --out data/phase4_counterfactuals.json
   ```
4. **Test:**  
   ```bash
   pytest tests/test_do_why_interface.py
   ```
5. **KPI:** Counterfactual time P95 ≤ 2 s; early-exit count ≤ 10%.

### 2.5 Timeout Manager Integration  
1. **File:** `src/timeout/timeout_manager.py`  
2. **Implement:**  
   - Provide decorator `@timeout(ms)` for functions in pruner, updater, ATE, and counterfactual.  
3. **Test:**  
   ```bash
   pytest tests/test_timeout_manager.py
   ```
4. **KPI:** No uncaught timeouts; logged via dashboard.

### 2.6 Causal Engine Orchestrator  
1. **File:** `src/causal_engine.py`  
2. **Implement:**  
   - Method `analyze(puzzle_id)` invokes Rung 1→1.5→2→3 sequentially.  
   - Applies early‐exit: skip Rung 2/3 if latency budget exceeded or variables  0.05 removed | Increase bootstrap samples; log for review |
| Counterfactual overruns     | Early-exit ≤ 10% puzzles | Lower sample size; coarse CF only             |
| Budget overshoot            | Full analysis ≤ 2 s      | Skip Rung 3 when necessary; degrade gracefully |

**Phase 4 Completion** is achieved when:  
- All unit tests in `tests/` pass.  
- CI schema validation reports zero errors.  
- Debug dashboard shows prune, intervention, and CF latencies within budgets.  
- End-to-end causal analysis on 200 puzzles meets MVI-2 accuracy and latency targets.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/6010715/f85bb9ce-0f1d-4083-86a7-d6a974339adb/projectreport.txt
[2] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/6010715/a758f4d2-5f83-45ed-9da1-f66cd767bb12/A-Hybrid-AI-Architecture-for-Solving-Bongard-Problems.docx


# Phase 5: Abstraction Mining & Concept Library – Detailed Implementation Guide

**Objective & Scope**  
Develop mid-level abstractions and curate a self-refining concept library on local hardware (zen 7, 16 GB RAM, RTX 3050 Ti). Achieve MVI-2 performance on 200 puzzles by extracting, clustering, and validating predicate-level schemas, analogical patterns, and relational hierarchies under a 2× relaxed latency budget (end-to-end abstraction cycle ≤ 60 s per puzzle).

## 1. Repository & Folder Structure  

```
bongard-solver/
├── data/
│   ├── phase5_embeddings.npy           # 32-D UMAP embeddings of each puzzle context
│   ├── phase5_graph_kernels.pkl        # WL kernel vectors for scene graphs
│   ├── phase5_schema_tree.json         # hierarchical concept clusters
│   └── phase5_analogy_pairs.json       # positive/negative pairs for analogy training
├── src/
│   ├── abstraction/
│   │   ├── graph_kernel.py             # computes WL graph kernel embeddings
│   │   ├── umap_reducer.py             # reduces fused feature vectors to 32 D
│   │   ├── concept_hierarchy.py        # agglomerative clustering & tree builder
│   │   ├── analogy_siamese_gnn.py      # defines & trains Siamese GNN
│   │   ├── hame_early_exit.py          # early-exit filter for partial embeddings
│   │   └── abstraction_miner.py        # orchestrates end-to-end extraction & promotion
│   ├── drift_monitor.py                # sliding-window drift detection on embeddings
│   └── concept_library.py              # load/query/persist curated concepts
├── scripts/
│   ├── compute_graph_kernels.py        # batch computes WL kernel vectors
│   ├── reduce_embeddings.py            # runs UMAP on fused features
│   ├── induce_hierarchy.py             # builds hierarchical clusters & novelty scores
│   ├── train_analogy_gnn.py            # trains Siamese GNN on analogy pairs
│   └── mine_abstractions.py            # end-to-end abstraction mining pipeline
├── tests/
│   ├── test_graph_kernel.py
│   ├── test_umap_reducer.py
│   ├── test_concept_hierarchy.py
│   ├── test_analogy_siamese_gnn.py
│   ├── test_hame_early_exit.py
│   └── test_concept_library.py
└── integration/
    └── debug_dashboard.py              # panels: cluster purity, drift alerts, analogy accuracy
```

## 2. Step-by-Step Implementation  

### 2.1 Fused Feature Embedding & UMAP Reduction  
1. **File**: `src/abstraction/umap_reducer.py`  
2. **Implement**:  
   - Load fused cues + Tiny-CLIP + physics + causal scores per puzzle (Phase 4 outputs).  
   - Use UMAP (n_components = 32, min_dist = 0.1) to embed into 32 D.  
3. **Script**:  
   ```bash
   python scripts/reduce_embeddings.py \
     --input data/phase4_fused_features.npy \
     --out data/phase5_embeddings.npy \
     --n-components 32
   ```
4. **Test**:  
   ```bash
   pytest tests/test_umap_reducer.py
   ```
5. **KPI**: Reduction time P95 ≤ 10 s; embedding drift RMS ≤ Δ_threshold.

### 2.2 Weisfeiler–Lehman Graph Kernels  
1. **File**: `src/abstraction/graph_kernel.py`  
2. **Implement**:  
   - Parse each puzzle’s scene graph (from Phase 3).  
   - Compute WL kernel vector (h=3 iterations) for graph similarity.  
3. **Script**:  
   ```bash
   python scripts/compute_graph_kernels.py \
     --input data/scene_graphs_physics.pkl \
     --out data/phase5_graph_kernels.pkl
   ```
4. **Test**:  
   ```bash
   pytest tests/test_graph_kernel.py
   ```
5. **KPI**: Kernel computation P95 ≤ 15 s per 200 puzzles bundle.

### 2.3 Hierarchical Concept Clustering  
1. **File**: `src/abstraction/concept_hierarchy.py`  
2. **Implement**:  
   - Load UMAP embeddings and graph kernels; concatenate per-puzzle vector (size 32 + K).  
   - Apply agglomerative clustering (Ward linkage) to form a tree; compute novelty = distance to nearest cluster centroid.  
   - Retain clusters where internal silhouette ≥ 0.6; annotate novelty scores.  
3. **Script**:  
   ```bash
   python scripts/induce_hierarchy.py \
     --embeddings data/phase5_embeddings.npy \
     --kernels data/phase5_graph_kernels.pkl \
     --out data/phase5_schema_tree.json
   ```
4. **Test**:  
   ```bash
   pytest tests/test_concept_hierarchy.py
   ```
5. **KPI**: Clustering time ≤ 30 s; cluster purity ≥ 0.7 on held-out 10%.

### 2.4 Contrastive Analogy Training (Siamese GNN)  
1. **File**: `src/abstraction/analogy_siamese_gnn.py`  
2. **Implement**:  
   - Build graph-based Siamese GNN that ingests scene graphs + embeddings.  
   - Train on positive/negative pairs (`phase5_analogy_pairs.json`) with margin ranking loss (margin=0.5).  
3. **Script**:  
   ```bash
   python scripts/train_analogy_gnn.py \
     --pairs data/phase5_analogy_pairs.json \
     --graphs data/scene_graphs_physics.pkl \
     --epochs 50 \
     --out model/analogy_gnn.pt
   ```
4. **Test**:  
   ```bash
   pytest tests/test_analogy_siamese_gnn.py
   ```
5. **KPI**: Validation accuracy ≥ 0.75; inference latency ≤ 100 ms per pair.

### 2.5 HAME Early-Exit Filtering  
1. **File**: `src/abstraction/hame_early_exit.py`  
2. **Implement**:  
   - For large-scale analogy or clustering, compute partial UMAP distance; if partial > threshold, skip full compute.  
3. **Test**:  
   ```bash
   pytest tests/test_hame_early_exit.py
   ```
4. **KPI**: Early-exit rate ≥ 40%; false-positive skip rate ≤ 5%.

### 2.6 Abstraction Miner Orchestration  
1. **File**: `src/abstraction/abstraction_miner.py`  
2. **Implement**:  
   - Pipeline: UMAP → WL kernels → clustering → analogy scoring → novelty tagging.  
   - Load drift monitor to compare new embeddings against running window; if drift > Z=2.0, retrigger reclustering.  
   - Persist curated concepts to `data/concept_library.json`.  
3. **Script**:  
   ```bash
   python scripts/mine_abstractions.py \
     --hierarchy data/phase5_schema_tree.json \
     --analogy-model model/analogy_gnn.pt \
     --out data/concept_library.json
   ```
4. **Test**:  
   ```bash
   pytest tests/test_concept_library.py
   ```
5. **KPI**: End-to-end abstraction mining P95 ≤ 60 s; drift monitor alerts ≤ 1/week.

## 3. Phase 5 Timeline (2 Months)  

| Week | Deliverable                                       | Validation                               |
|------|---------------------------------------------------|------------------------------------------|
| 1    | UMAP reducer integration & tests                  | `test_umap_reducer.py`; ≤ 10 s           |
| 2    | WL graph kernel computation & tests               | `test_graph_kernel.py`; ≤ 15 s bundle    |
| 3    | Concept hierarchy induction & tests               | `test_concept_hierarchy.py`; ≥ 0.7 silhouette |
| 4    | Train & validate Siamese GNN analogy model        | `test_analogy_siamese_gnn.py`; ≥ 0.75 acc |
| 5    | HAME early-exit module & tests                    | `test_hame_early_exit.py`; ≥ 40% skip    |
| 6    | AbstractionMiner orchestration & library output    | `test_concept_library.py`; JSON schema OK |
| 7    | Drift monitor hooks & reclustering logic          | Drift alerts tested; ≤ 1/week            |
| 8    | Dashboard panels & CI schema validation           | `integration/debug_dashboard.py`; CI green |

## 4. Key KPIs & Risk Mitigation  

| Risk                              | KPI Target                         | Mitigation                                     |
|-----------------------------------|------------------------------------|------------------------------------------------|
| Embedding drift                   | Z-score ≤ 2.0                      | Retrigger reclustering; archive old concepts   |
| Clustering overfit                | Silhouette ≥ 0.6                   | Hold-out validation; prune small clusters      |
| Analogy model underperformance    | Acc ≥ 0.75                         | Augment pairs; adjust margin & learning rate   |
| Early-exit false skips            | FP rate ≤ 5%                       | Tune threshold; fallback full compute          |
| Pipeline latency overshoot        | ≤ 60 s per puzzle (P95)           | Profile hotspots; parallelize                      |

**Phase 5 Completion** is achieved when:  
1. All unit tests in `tests/` pass.  
2. CI schema checks for `concept_library.json` pass.  
3. Debug dashboard shows cluster purity ≥ 0.7, analogy acc ≥ 0.75, drift within threshold, and overall abstraction mining within time budgets.  
4. A curated concept library is ready for downstream Phase 6 refinement.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/6010715/f85bb9ce-0f1d-4083-86a7-d6a974339adb/projectreport.txt
[2] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/6010715/a758f4d2-5f83-45ed-9da1-f66cd767bb12/A-Hybrid-AI-Architecture-for-Solving-Bongard-Problems.docx


# Phase 6: Nested RL-MCTS Refinement – Detailed Implementation Guide  

**Objective & Scope**  
Elevate the Bongard-Solver with a nested, elegance- and causal-aware Monte Tree Search (MCTS) refinement on local hardware (Ryzen 7, 16 GB RAM, RTX 3050 Ti). Achieve ≥85% accuracy on a 100-puzzle subset (MVI-3) within 2× relaxed latency budgets (per-puzzle refinement ≤2 s). Integrate policy/value networks, elegance penalties, nested AST composers, and dynamic grammar triggers under robust profiling and CI contracts.

## 1. Repository & Folder Structure  

```
bongard-solver/
├── src/
│   ├── rl_mcts/
│   │   ├── state_encoder.py          # Encodes puzzle context into MCTS state
│   │   ├── elegance_uct.py           # UCT extension with elegance & prior biases
│   │   ├── nested_composer.py        # Merges partial rule ASTs into nested trees
│   │   ├── policy_value_net.py       # Defines & loads MLP policy/value models
│   │   └── rl_mcts.py                # Main MCTS loop with nested actions
│   ├── grammar_extender.py           # Triggers DSL growth on novelty cues
│   └── utils/
│       ├── timeout_manager.py        # Per-rollout deadlines enforcement
│       └── profiling.py              # TaskProfiler hooks
├── config/
│   └── rl_config.yaml                # Hyperparameters: rollout budgets, penalties
├── scripts/
│   └── run_rl_mcts.py                # Orchestrates per-puzzle refinement
├── tests/
│   ├── test_state_encoder.py
│   ├── test_elegance_uct.py
│   ├── test_nested_composer.py
│   ├── test_policy_value_net.py
│   ├── test_rl_mcts.py
│   └── test_timeout_manager.py
└── integration/
    └── debug_dashboard.py            # Panels: rollouts, Q-values, grammar triggers
```

## 2. Step-by-Step Implementation  

### 2.1 State Encoding  
1. **File:** `src/rl_mcts/state_encoder.py`  
2. **Implement:**  
   - Combine: Phase 5 UMAP embeddings (32 D), scene-graph WL kernels (K D), causal scores, cue-entropy, rule-depth, grammar complexity.  
   - Output fixed-size tensor for policy/value nets.

### 2.2 Policy/Value Networks  
1. **File:** `src/rl_mcts/policy_value_net.py`  
2. **Implement:**  
   - MLP with two hidden layers (128→64) taking state vector; outputs:  
     - Policy logits over DSL actions.  
     - State-value scalar.  
   - Load quantized INT4 weights via GPTQ.

### 2.3 Elegance-Aware UCT Formula  
1. **File:** `src/rl_mcts/elegance_uct.py`  
2. **Implement:**  
   - UCT score =  
     $$
     \frac{Q_i}{N_i} + c\sqrt{\frac{\ln N_p}{N_i}} - \lambda \frac{\text{AST\_size}_i}{\text{max\_size}}
     $$  
   - Prior bias term from policy logits.  
   - Elegance penalty λ from `rl_config.yaml`.

### 2.4 Nested Rule Composer  
1. **File:** `src/rl_mcts/nested_composer.py`  
2. **Implement:**  
   - “Merge” action: combine two partial AST nodes under quantifier or logical operator.  
   - Enforce max depth = 3.  
   - Timeout guard: rollback if compose > 100 ms.

### 2.5 MCTS Main Loop  
1. **File:** `src/rl_mcts/rl_mcts.py`  
2. **Implement:**  
   - For each puzzle:  
     1. Encode state.  
     2. Run rollouts (budget 500 rollouts or 200 ms per rollout) in parallel threads (max 4).  
     3. At each node, select via `elegance_uct`, expand via composer, evaluate leaf via value net.  
     4. Backup values.  
     5. If leaf introduces novel predicate beyond coverage threshold, call `grammar_extender.trigger()`, update DSL and restart rollout.  
   - Early-exit: if best-found rule F1 > threshold, halt.

### 2.6 Grammar Extender Integration  
1. **File:** `src/grammar_extender.py`  
2. **Implement:**  
   - On novelty trigger: propose up to 3 new primitive predicates via template heuristics.  
   - Validate proposals on held-out splits; integrate highest-scoring.

### 2.7 Orchestration Script  
1. **File:** `scripts/run_rl_mcts.py`  
2. **Implement:**  
   ```bash
   python scripts/run_rl_mcts.py \
     --puzzles data/phase5_schema_tree.json \
     --models model/policy_value_int4.pt \
     --config config/rl_config.yaml \
     --out data/phase6_refined_rules.json
   ```
3. **Features:** batch puzzles, per-puzzle log of rollouts and chosen rule.

### 2.8 Timeout & Profiling  
- Use `utils/timeout_manager.py` to decorate per-rollout and composer calls.  
- Insert `profiling` hooks at: state encoding, selection, expansion, evaluation, backup.  
- Expose metrics to `integration/debug_dashboard.py`.

## 3. Phase 6 Timeline (3 Months)  

| Week | Deliverable                                 | Validation                              |
|------|---------------------------------------------|-----------------------------------------|
| 1–2  | StateEncoder & Policy/ValueNet modules      | `test_state_encoder.py` green; dims match config |
| 3    | Implement EleganceUCT & associated tests    | `test_elegance_uct.py`; correct UCT scores       |
| 4    | NestedComposer & timeout guards             | `test_nested_composer.py`; < 100 ms compose     |
| 5–6  | MCTS loop core with parallel rollouts       | `test_rl_mcts.py`; 500 rollouts < 2 s/puzzle      |
| 7    | GrammarExtender triggers & DSL update       | integration run shows DSL growth on novel cases |
| 8    | Orchestration script & end-to-end smoke test| 100 puzzles refined under budget          |
| 9    | Dashboard panels & profiling integration    | metrics: rollouts, Q-values, grammar events     |
| 10–12| CI schema contracts, full regression tests  | All tests pass; CI green; coverage ≥90%         |

## 4. Key KPIs & Risk Mitigation  

| Risk                             | KPI Target                 | Mitigation                                     |
|----------------------------------|----------------------------|------------------------------------------------|
| Rollout latency overshoot        | 95% ≤ 2 s/puzzle           | Early-exit; reduce rollouts; lower depth       |
| Composer timeouts                | < 1% rollouts              | Pre-check AST size; lower parallel threads     |
| Policy/value net OOM             | GPU mem < 3.8 GB           | Quantized INT4; move select layers to CPU      |
| Grammar drift explosion          | ≤ 5 new predicates/session | Hard limit & CI validation                     |
| Profiling overhead               | < 5% runtime              | Lightweight hooks; sample profiles every 10 puzzles |

**Phase 6 Completion** when:  
- All unit and integration tests pass.  
- CI schema validation for `phase6_refined_rules.json`.  
- Debug dashboard confirms rollouts and grammar triggers within budgets.  
- End-to-end MVI-3 slice (100 puzzles) ≥85% accuracy under 2 s average.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/6010715/f85bb9ce-0f1d-4083-86a7-d6a974339adb/projectreport.txt
[2] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/6010715/a758f4d2-5f83-45ed-9da1-f66cd767bb12/A-Hybrid-AI-Architecture-for-Solving-Bongard-Problems.docx


# Phase 7: Meta-Controller & Antifragile Orchestration – Detailed Implementation Guide

**Objective & Scope**  
Implement a master orchestration layer that dynamically selects and tunes solving strategies puzzle, rapidly adapts via meta-learning, coordinates expert feedback on ambiguous cases, and drives continual self-improvement. Targets MVI-3 on a 100-puzzle subset: ≥85% accuracy within 30 s per puzzle on local hardware (Ryzen 7, 16 GB RAM, RTX 3050 Ti).

## 1. Repository & Folder Structure

```
bongard-solver/
├── data/
│   ├── phase7_bandit_logs.csv         # fingerprint, strategy, reward history
│   ├── phase7_maml_checkpoints/       # saved meta-parameter snapshots
│   └── phase7_expert_queue.json       # puzzles flagged for review
├── src/
│   ├── meta/
│   │   ├── fingerprint_extractor.py   # builds context vectors per puzzle
│   │   ├── neural_bandit.py           # 2-layer contextual bandit network
│   │   ├── maml_adaptor.py            # MAML inner/outer loops
│   │   ├── meta_controller.py         # selects strategies and updates bandit
│   │   ├── orchestrator.py            # dispatches to solvers and collects results
│   │   ├── expert_collab.py           # manages expert-in-the-loop queue
│   │   ├── self_improver.py           # detects drift/error and retrains modules
│   │   └── apoe.py                    # Adaptive Performance Optimizer
│   └── interactive/
│       └── human_loop.py              # web widget for expert judgments
├── config/
│   └── phase7.yaml                    # hyperparameters: learning rates, thresholds
├── scripts/
│   └── run_phase7.py                  # end-to-end Phase 7 orchestration
├── tests/
│   ├── test_fingerprint_extractor.py
│   ├── test_neural_bandit.py
│   ├── test_maml_adaptor.py
│   ├── test_meta_controller.py
│   ├── test_orchestrator.py
│   ├── test_expert_collab.py
│   ├── test_self_improver.py
│   ├── test_apoe.py
│   └── test_human_loop.py
└── integration/
    └── debug_dashboard.py            # panels: strategy metrics, resource shifts, expert queue
```

## 2. Step-by-Step Implementation

### 2.1 Fingerprint Extraction  
1. **File:** `src/meta/fingerprint_extractor.py`  
2. **Implement:**  
   - Inputs: per-puzzle Phase 6 outputs (cue_entropy, rule_depth, causal_edges, RL_confidence, abstraction_novelty, grounding_latency).  
   - Construct normalized feature vector of length 8.  
   - Append current macro-risk Z-scores from KPIs.  
   - Output: `phase7_bandit_logs.csv` rows: puzzle_id, fingerprint, timestamp.  
3. **Test:**  
   ```bash
   pytest tests/test_fingerprint_extractor.py
   ```

### 2.2 Contextual Bandit Network  
1. **File:** `src/meta/neural_bandit.py`  
2. **Implement:**  
   - Two-layer MLP (128→64 hidden) with softmax head producing probabilities over 5 strategies:  
     * cue-only, grammar, causal+grammar, MCTS-refine, ask-LLM  
   - Cross-entropy loss with reward shaping.  
   - Use quantized INT8 weights for CPU inference.  
3. **Test:**  
   ```bash
   pytest tests/test_neural_bandit.py
   ```

### 2.3 MAML Adaptor  
1. **File:** `src/meta/maml_adaptor.py`  
2. **Implement:**  
   - Inner loop: 5 gradient steps on last 20 logged fingerprints, lr=1e-4.  
   - Outer loop: update meta-parameters via first-order MAML after each batch of 100 puzzles.  
   - Save checkpoints in `phase7_maml_checkpoints/`.  
3. **Test:**  
   ```bash
   pytest tests/test_maml_adaptor.py
   ```

### 2.4 Meta-Controller Core  
1. **File:** `src/meta/meta_controller.py`  
2. **Implement:**  
   - Method `select_strategy(fingerprint)` → (strategy, prob_dist).  
   - After puzzle solves, receive reward (+1 correct within 30 s, –0.1 per second over 30 s).  
   - Update bandit via gradient step.  
   - Periodically call MAML adaptor to refresh weights.  
3. **Test:**  
   ```bash
   pytest tests/test_meta_controller.py
   ```

### 2.5 Orchestrator Dispatcher  
1. **File:** `src/meta/orchestrator.py`  
2. **Implement:**  
   - Routes puzzles to appropriate solver module:  
     * ILPSolver, CodeletSystem, CausalEngine, RL_MCTS, Multimodal CoT  
   - Monitors execution time; enforces fallback to System-1 if solver fails or times out.  
   - Returns rule, confidence, latency.  
3. **Test:**  
   ```bash
   pytest tests/test_orchestrator.py
   ```

### 2.6 Expert Collaboration Orchestrator (ECO)  
1. **File:** `src/meta/expert_collab.py` & `src/interactive/human_loop.py`  
2. **Implement:**  
   - Criteria: puzzles with bandit confidence  ε.  
   - Trigger targeted retraining: e.g., re-extract cues, re-fine DSL, retrain RL nets.  
   - Log actions to `phase7_bandit_logs.csv`.  
3. **Test:**  
   ```bash
   pytest tests/test_self_improver.py
   ```

### 2.8 Adaptive Performance Optimizer (APOE)  
1. **File:** `src/meta/apoe.py`  
2. **Implement:**  
   - Read HIC and TaskProfiler metrics.  
   - Dynamically reallocate CPU/GPU threads to bottlenecked modules.  
   - Adjust early-exit thresholds when resource contention arises.  
3. **Test:**  
   ```bash
   pytest tests/test_apoe.py
   ```

### 2.9 End-to-End Orchestration Script  
1. **File:** `scripts/run_phase7.py`  
2. **Implement:**  
   ```bash
   python scripts/run_phase7.py \
     --puzzles data/phase6_refined_rules.json \
     --config config/phase7.yaml \
     --out data/phase7_results.json
   ```
   - Performs fingerprint extraction → strategy selection → solver dispatch → reward update → meta-learning → self-improvement → expert queueing → resource optimization.  
3. **Smoke Test:**  
   ```bash
   pytest tests/test_orchestrator.py::test_end_to_end
   ```

## 3. Phase 7 Timeline (3 Months)

| Week     | Deliverable                                                              |
|----------|---------------------------------------------------------------------------|
| 1–2      | FingerprintExtractor & NeuralBandit prototype; bandit training on logs    |
| 3–4      | MAML adaptor integration; checkpointing and meta-parameter refresh         |
| 5        | MetaController and Orchestrator core; strategy selection and dispatch     |
| 6        | ExpertCollab queue and human_loop widget; integrate expert feedback loop  |
| 7        | SelfImprover routines; validate targeted retraining triggers              |
| 8        | APOE resource optimizer; dynamic thread reallocation under load           |
| 9        | Orchestration script completed; end-to-end Phase 7 smoke test on 100 puzzles |
| 10–12    | Full unit/integration tests; CI schema validation; debug dashboard panels; documentation and release |

## 4. Key KPIs & Risk Mitigation

| Risk                               | KPI Target                                | Mitigation                                                  |
|------------------------------------|-------------------------------------------|-------------------------------------------------------------|
| Bandit misallocation               | >60% puzzles routed optimally            | Increase exploration ε; retrain bandit with new data        |
| MAML overfitting                   | Meta-loss Δ ≤ 0.02 over 100 puzzles       | Add regularization; limit inner-loop steps                  |
| Expert queue backlog               | <5 puzzles awaiting review               | Prioritize high-impact puzzles; throttle expert invitations |
| Self-improver instability          | Retrain success rate ≥ 80%                | Validate modules pre-retrain; rollback on failure           |
| Resource thrashing                 | Utilization variance < 10%                | APOE early-exit; cap thread reassignments                   |

**Phase 7 Completion** is achieved when:  
- End-to-end run on 100 puzzles yields ≥85% accuracy within 30 s.  
- Bandit improves strategy success rate by ≥10% vs. uniform baseline.  
- Expert feedback cycles refine ≥5 grammar rules.  
- All tests pass and CI reports zero schema errors.  
- Debug dashboard displays stable resource allocations and low expert-queue latency.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/6010715/f85bb9ce-0f1d-4083-86a7-d6a974339adb/projectreport.txt
[2] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/6010715/a758f4d2-5f83-45ed-9da1-f66cd767bb12/A-Hybrid-AI-Architecture-for-Solving-Bongard-Problems.docx
